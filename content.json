{"pages":[{"title":"About","date":"2019-02-06T02:53:57.077Z","path":"about/index.html","text":"佛曰：不可说"},{"title":"Tags","date":"2019-02-06T02:53:57.079Z","path":"tags/index.html","text":""},{"title":"Categories","date":"2019-02-06T02:53:57.077Z","path":"categories/index.html","text":""}],"posts":[{"title":"MySQL 主从模式","date":"2019-01-07T13:00:00.000Z","path":"wiki/技术开发/存储/mysql/MySQL 主从模式/","text":"MySQL 支持配置 master-slave 模式，slave 从 master 定期同步数据，但不支持自动选举。常见的架构方式有：一主一从、一主多从(提升读性能)、多主一从(收集数据统计)等。 环境准备: 安装两台 MySQL-5.7 服务. 1. Master 节点配置 创建用于数据同步的 mysql 用户create user repl identified by &#39;repl&#39;; 为创建的用户授予同步权限grant replication slave on *.* to &#39;repl&#39;@&#39;%&#39; identified by &#39;repl&#39;; 修改配置文件 /etc/my.cnf, 开启binlog日志文件同步 123456789101112131415161718192021[mysqld]# 表示服务为一IDserver-id=104# 开启日志同步, 设置日志文件名称log_bin=mysql-bin# 日志文件记录方式, 默认是 ROW 记录变更内容, STATEMENT 记录语句, MIXED 为二者混合方式binlog_format=MIXED# 日志文件同步周期, 0表示交给内核处理同步, N表示事务数sync_binlog=1# 日志文件自动删除天数, 默认为0表示不自动删除expire_logs_days=7# 不进行同步的数据库binlog_ignore_db=mysqlbinlog_ignore_db=information_schemabinlog_ignore_db=performation_schemabinlog_ignore_db=sys 重启数据库, 查询 master 信息 1234567mysql&gt; show master status;+------------------+----------+--------------+--------------------------------------------------+-------------------+| File | Position | Binlog_Do_DB | Binlog_Ignore_DB | Executed_Gtid_Set |+------------------+----------+--------------+--------------------------------------------------+-------------------+| mysql-bin.000001 | 154 | | mysql,information_schema,performation_schema,sys | |+------------------+----------+--------------+--------------------------------------------------+-------------------+1 row in set (0.00 sec) 2. Slave 节点配置 修改配置文件 /etc/my.cnf, 开启binlog同步 123456789101112[mysqld]# 服务节点唯一IDserver-id=110# 同步日志文件名称relay-log=slave-relay-bin# 同步日志索引relay-log-index=slave-relay-bin.index# 是否只读read_only=1 重启并连接到数据库systemctl restart mysqld &amp;&amp; mysql -uroot -proot 建立 master-slave 连接, 多主一从的情况可以执行多个 change master 指向不同主节点 1234567# master_host 主节点# master_port 主节点端口# master_user 创建的用于同步数据的用户# master_password 密码# master_log_file binlog文件由主节点获得# master_log_pos binlog开始同步位置change master to master_host=&apos;192.168.56.104&apos;,master_port=3306,master_password=&apos;repl&apos;,master_user=&apos;repl&apos;,master_log_file=&apos;mysql-bin.000001&apos;,master_log_pos=154; 开启同步 start slave; 查看 slave 运行状态123456789101112131415mysql&gt; show slave status\\G;*************************** 1. row *************************** Slave_IO_State: Connecting to master Master_Host: 192.168.56.104 Master_User: repl Master_Port: 3306 Connect_Retry: 60 Master_Log_File: mysql-bin.000001 Read_Master_Log_Pos: 154 Relay_Log_File: slave-relay-bin.000001 Relay_Log_Pos: 4 Relay_Master_Log_File: mysql-bin.000001 Slave_IO_Running: Yes Slave_SQL_Running: Yes ... 运行状态中 Slave_IO_Running: Yes, Slave_SQL_Running: Yes 两个线程运行成功, 则表示主从同步成功了. 3. 主从同步的原理 master记录二进制日志。在每个事务更新数据完成之前，master在二日志记录这些改变。MySQL将事务串行的写入二进制日志，即使事务中的语句都是交叉执行的。在事件写入二进制日志完成后，master通知存储引擎提交事务 slave将master的binary log拷贝到它自己的中继日志。首先，slave开始一个工作线程——I/O线程。I/O线程在master上打开一个普通的连接，然后开始binlog dump process。Binlog dump process从master的二进制日志中读取事件，如果已经跟上master，它会睡眠并等待master产生新的事件。I/O线程将这些事件写入中继日志 SQL线程从中继日志读取事件，并重放其中的事件而更新slave的数据，使其与master中的数据一致 4. binlog 的格式 mysql 使用二进制文件binlog记录数据更新或者潜在的更新, 存储在 /var/lib/mysql 目录下. 查询binlog格式123456mysql&gt; show variables like '%binlog%';+-----------------------------------------+----------------------+| Variable_name | Value |+-----------------------------------------+----------------------+| binlog_format | MIXED |+-----------------------------------------+----------------------+ statement : 基于sql语句的存储方式. 无法记录包含函数(uuid, now, other fun). row : 基于行内容, 记录修改后每条记录变化的值. mixed : 混合模式, 由mysql判断处理. 设置binlog格式 修改配置文件中的 binlog_format=&#39;row&#39;选项 连接到mysql, 修改全局配置 set global binlog_format=’rowt’; 查看binlog内容mysql 提供了查看二进制binlog文件的工具1mysqlbinlog --base64-output=decode-rows -v mysql-bin.000001","tags":[{"name":"mysql","slug":"mysql","permalink":"https://joecnn.github.io/wiki-site/tags/mysql/"}],"categories":[{"name":"技术开发","slug":"技术开发","permalink":"https://joecnn.github.io/wiki-site/categories/技术开发/"},{"name":"存储","slug":"技术开发/存储","permalink":"https://joecnn.github.io/wiki-site/categories/技术开发/存储/"},{"name":"mysql","slug":"技术开发/存储/mysql","permalink":"https://joecnn.github.io/wiki-site/categories/技术开发/存储/mysql/"}]},{"title":"MySQL quickstart","date":"2019-01-07T09:00:00.000Z","path":"wiki/技术开发/存储/mysql/MySQL quickstart/","text":"一、安装 MySQL-5.7 下载 MySQL-5.7 的 repo 源：wget http://repo.mysql.com/mysql57-community-release-el7.rpm 安装源：rpm -ivh mysql57-community-release-el7.rp 安装数据库：yum -install mysql-server 启动数据库：systemctl start mysqld 安装后文件对应的目录 mysql 的数据和二进制文件：/var/lib/mysql mysql 的配置文件：/etc/my.cnf mysql 的日志文件：/var/log/mysql.log 二、登录到 MySQLMySQL-5.7 版本对新安装的 root 账号有一个随机密码，可以通过 grep &quot;password&quot; /var/log/mysqld.log 获得，root@localhost 此处为随机密码 运行 mysql -uroot -p 输入初始的随机密码进入 设置密码 默认的随机密码无法对数据库进行操作，需要进行设置，而5.7版本用了validate_password密码加强插件，简单的密码无法通过验证 执行以下两条命令，让密码可以随意设置 12set global validate_password_length=1;set global validate_password_policy=0; 这样可以设置简单的密码，但是长度要求大于4位set password=password(&quot;root&quot;) 连接授权默认情况下其它服务器的客户端不能直接访问mysql服务器，需要对IP授权。GRANT ALL PRIVILEGES ON *.* TO &#39;root&#39;@&#39;%&#39; IDENTIFIED BY &#39;root&#39; WITH GRANT OPTION; 验证查询所有的数据库 show databases;","tags":[{"name":"mysql","slug":"mysql","permalink":"https://joecnn.github.io/wiki-site/tags/mysql/"}],"categories":[{"name":"技术开发","slug":"技术开发","permalink":"https://joecnn.github.io/wiki-site/categories/技术开发/"},{"name":"存储","slug":"技术开发/存储","permalink":"https://joecnn.github.io/wiki-site/categories/技术开发/存储/"},{"name":"mysql","slug":"技术开发/存储/mysql","permalink":"https://joecnn.github.io/wiki-site/categories/技术开发/存储/mysql/"}]},{"title":"Redis 集群","date":"2018-12-30T16:00:00.000Z","path":"wiki/技术开发/分布式/Redis 集群/","text":"Redis 是一个开源的 key-value 存储系统，由于出众的性能，大部分互联网企业都用来做服务器端缓存。Redis 在3.0版本前只支持单实例模式，虽然支持主从模式、哨兵模式部署来解决单点故障，但是现在互联网企业动辄大几百G的数据，可完全是没法满足业务的需求，所以，Redis 在 3.0 版本以后就推出了集群模式。 一、Master-slave 模式实现主从复制模式，只需要在slave服务器上修改配置文件：12## 配置master服务器slaveof &lt;masterip&gt; &lt;masterport&gt; slave服务器只接收读请求，可以用来做读写分离，通过sync命令向master同步数据。 配置完成后启动，可以通过命令查看状态：12## 输出当前服务信息info replication 数据同步从节点定时(1S)从主节点同步数据，通过发送sync命令, 通过命令可以监控同步信息： 1replconf listening –port 6379 可以使用命令手动同步数据：sync. 数据同步方式 基于RDB文件的复制(第一次连接或重启的时候) 无硬盘复制 repl-diskless-sync yes 增量复制 PSYNC master run id. Offset 实现原理 slave 第一次或者重连到 master 上以后，会向 master 发送一个SYNC的命令 master 收到SYNC的时候，会做两件事： 执行bgsave（rdb的快照文件） master 把新收到的修改命令存入到缓冲区, 通过RESP协议发送给 slave (aof方式) slave 收到文件后会先清空数据，再从rdb快照文件恢复 缺点：主从复制的模式无法对 master 进行动态选举。 二、哨兵模式Redis 提供了哨兵工具，监控 master 和 salve 是否正常运行,如果 master 出现故障，那么会把其中一台 salve 数据升级为 master。 哨兵机也可做集群防止单点问题，启动哨兵的步骤： 拷贝哨兵配置文件 sentinel.conf 修改配置文件 12## 配置主节点名称、ip、port、master需要的投票数sentinel monitor &lt;master-name&gt; &lt;ip&gt; &lt;redis-port&gt; &lt;quorum&gt; 启动哨兵 1bin/redis-sentinel sentinel.conf 三、Redis 集群Redis 集群采用了P2P的模式，完全去中心化。Redis 把所有的 Key 分成了 16384 个 slot，每个 Redis 实例负责其中一部分 slot 。集群中的所有信息（节点、端口、slot等），都通过节点之间定期的数据交换而更新。Redis 客户端可以在任意一个 Redis 实例发出请求，如果所需数据不在该实例中，通过重定向命令引导客户端访问所需的实例。 修改配置文件，采用集群方式：1234567891011## 节点标识pid 6379和port要对应pidfile /var/run/redis_6379.pid## 启动集群模式cluster-enabled yes## 集群配置文件cluster-config-file nodes-6379.conf## 集群节点连接超时cluster-node-timeout 15000 启动每个redis服务，尝试使用命令，发现集群还无法使用，提示信息： 1(error) CLUSTERDOWN Hash slot not served Redis 节点虽然启动了，但是它们之间还无法相互发现，也无法分配slot，需要一个中间人调度。 安装集群所需软件由于 Redis 集群需要使用 ruby 命令，所以我们需要安装 ruby 和相关接口。 123yum install rubyyum install rubygemsgem install redis 调用 ruby 命令创建集群： 1bin/redis-trib.rb create --replicas 1 192.168.56.110:6379 192.168.56.110:6380 192.168.56.120:6379 192.168.56.120:6380 192.168.56.130:6379 192.168.56.130:6380 --replicas 1 表示主从复制比例为 1:1, 所以这里需要有6个 Redis 服务节点，组成3主3从的集群。 验证一下依然是通过客户端命令连接上，通过集群命令看一下状态和节点信息等。 123bin/redis-cli -c -h 192.168.56.110 -p 6379cluster infocluster nodes 四、其它集群方案一致性哈希通过程序对Key进行一致性哈希，再路由到不同的 Redis 服务节点上。 redis-sharddingJedis封装的基于一致性哈希算法的解决方案，通过ShareddingJedis客户端连接到服务节点，也是在应用层面实现的。 codis基于 redis-2.8 开发的 codis-server，支持了数据的分片存储，通过codis-proxy实现请求路由。 twemproxytwitter 提供的解决方案，也是通过增加 proxy 代理层，做数据分片存储和请求路由。","tags":[{"name":"分布式","slug":"分布式","permalink":"https://joecnn.github.io/wiki-site/tags/分布式/"},{"name":"redis","slug":"redis","permalink":"https://joecnn.github.io/wiki-site/tags/redis/"}],"categories":[{"name":"技术开发","slug":"技术开发","permalink":"https://joecnn.github.io/wiki-site/categories/技术开发/"},{"name":"分布式","slug":"技术开发/分布式","permalink":"https://joecnn.github.io/wiki-site/categories/技术开发/分布式/"}]},{"title":"Redis 持久化机制","date":"2018-12-30T16:00:00.000Z","path":"wiki/技术开发/分布式/Redis 持久化机制/","text":"Redis 是一款基于内存的键值对数据库，使用单线程IO多路复用技术达到高性能。但某些场景下对数据的持久性是有要求的，所以Rdis提供了两种持久化策略，防止宕机数据丢失。 Redis 持久化机制对于persistence持久化存储，Redis提供了两种方式： RDB(Redis database) 存储快照文件snapshot AOF(Append-only file) 存储redo文件 1. RDBRBD 是在规定时间点将内存数据通过快照方式写入临时文件，再替换上次的持久化文件，达到数据持久化的方式。 优点： 使用fork出的子进程处理，不影响主进程 输出snapshot快速且体积小缺点： RDB间隔一段时间执行，如果在执行期间发生故障，将导致数据丢失 当内存数据较大时，fork操作将花费较长时间，导致Redis无法提供服务 RDB 会在指定的情况下触发快照 配置的快照间隔时间 12345678910111213141516## RDB默认是开启的## RDB文件名dbfilename dump.db## RDB文件存储目录dir ./## RDB快照触发时机, save &lt;间隔时间&gt; &lt;操作数&gt;## 全部删除即关闭RDBsave 900 1## RDB快照异常时, 是否阻塞客户端\"变更操作\"stop-writes-on-bgsave-error yes## RDB文件是否进行压缩rdbcompression yes 执行 save 或 bgsave 时 12345## 同步进行./redis-cli -h ip -p port save## 异步进行./redis-cli -h ip -p port bgsave 执行 flushall 时 执行master/slave全量复制时 快照的实现原理 Redis使用fork复制一份当前进程的副本(子进程)。 父进程继续接收处理客户端请求，子进程开始将内存的数据写入到临时文件。 子进程用临时文件替换原文件。 注意：redis在进行快照的过程中不会修改RDB文件，只有快照结束后才会将旧的文件替换成新的，也就是说任何时候RDB文件都是完整的。 这就使得我们可以通过定时备份RDB文件来实现redis数据库的备份， RDB文件是经过压缩的二进制文件，占用的空间会小于内存中的数据，更加利于传输。 2. AOFAOF 将”操作+数据”以格式化(RESP)的方式最佳到操作日志文件尾部，在append操作成功后才进行实际数据的变更。当Server需要恢复时，可以直接replay日志文件，即可还原所有操作过程。 优点： 可以保持更高的数据完整性，如果设置间隔1S则最多丢失1S的数据。 可以手动删除其中某些命令，方便维护缺点： 额外的IO操作，略微影响Redis性能 AOF文件比RDB文件大 恢复速度慢，要逐条重放命令 AOF 默认关闭，开启时需要修改配置文件；12345## 开启aofappendonly yes## 指定aof文件名称appendfilename appendonly.aof AOF 文件重写在开启AOF时，命令会一直append到aof文件中，使得aof文件体积越来越大，Redis支持对aof文件进行重写(rewrite)，合并相同Key的操作，保留最小命令集合。1234567##aof文件rewrite触发的最小文件尺寸(mb,gb),只有大于此aof文件大于此尺寸是才会触发rewrite，默认“64mb”，建议“512mb” auto-aof-rewrite-min-size 64mb ##相对于“上一次”rewrite，本次rewrite触发时aof文件应该增长的百分比。 ##每一次rewrite之后，redis都会记录下此时“新aof”文件的大小(例如A)，那么当aof文件增长到A*(1 + p)之后 ##触发下一次rewrite，每一次aof记录的添加，都会检测当前aof文件的尺寸。 auto-aof-rewrite-percentage 100 整个重写操作是绝对安全的，因为 Redis 在创建新 AOF 文件的过程中，会继续将命令追加到现有的 AOF 文件里面，即使重写过程中发生停机，现有的 AOF 文件也不会丢失。 而一旦新 AOF 文件创建完毕，Redis 就会从旧 AOF 文件切换到新 AOF 文件，并开始对新 AOF 文件进行追加操作。AOF 文件有序地保存了对数据库执行的所有写入操作， 这些写入操作以 Redis 协议的格式保存， 因此 AOF 文件的内容非常容易被人读懂， 对文件进行分析（parse）也很轻松。 同步磁盘数据 Redis每次更改数据的时候都会将命令记录到aof文件，但是实际上由于kernel的缓存机制，数据并没有实时写入到硬盘，而是进入硬盘缓存,再通过硬盘缓存机制去刷新到保存到文件。 12345678 ## 每次执行写入都会进行同步 ， 这个是最安全但是是效率比较低的方式 # appendfsync always ## 每一秒执行appendfsync everysec ## 不主动进行同步操作，由操作系统去执行，这个是最快但是最不安全的方式# appendfsync no aof文件损坏后怎么恢复 Redis 在执行命令中途宕机，导致命令只存储到aof一半，这个时候通过aof文件无法恢复，需要先对文件进行修复。 aof文件修复可以使用Redis提供的工具： 12## bin/redis-check-aof --fix 3. RDB和AOF如何选择 一般来说,如果对数据的安全性要求非常高的话，应该同时使用两种持久化功能。如果可以承受数分钟以内的数据丢失，那么可以只使用 RDB 持久化。有很多用户都只使用 AOF 持久化， 但并不推荐这种方式： 因为定时生成 RDB 快照（snapshot）非常便于进行数据库备份， 并且 RDB 恢复数据集的速度也要比 AOF 恢复的速度要快 。两种持久化策略可以同时使用，也可以使用其中一种。如果同时使用的话， 那么Redis重启时，会优先使用AOF文件来还原数据。","tags":[{"name":"分布式","slug":"分布式","permalink":"https://joecnn.github.io/wiki-site/tags/分布式/"},{"name":"redis","slug":"redis","permalink":"https://joecnn.github.io/wiki-site/tags/redis/"}],"categories":[{"name":"技术开发","slug":"技术开发","permalink":"https://joecnn.github.io/wiki-site/categories/技术开发/"},{"name":"分布式","slug":"技术开发/分布式","permalink":"https://joecnn.github.io/wiki-site/categories/技术开发/分布式/"}]},{"title":"Redis quickstart","date":"2018-12-28T16:00:00.000Z","path":"wiki/技术开发/分布式/Redis quickstart/","text":"1. Redis 是什么？ Redis is an open source (BSD licensed), in-memory data structure store, used as a database, cache and message broker. Redis 是一个开源的分布式键值对(key/value)存储数据库，常用于做为数据缓存、单点登录、网站访问排名、秒杀抢购、应用模块开发等。 2. 安装 Redis 下载 redis-4.0.12.tar.gz 安装包 解压并进行编译测试make &amp;&amp; make test 安装到指定目录cd src &amp;&amp; make install PREFIX=/usr/local/redis-4.0.12 make install安装完成后会在指定目录下生成bin/文件夹，里面存放着使用 redis 的工具。 3. 启动和停止 Redis 首先到源目录下拷贝配置文件到安装目录cp ~/redis-4.0.12/redis.conf /usr/local/redis-4.0.12/ 修改配置信息 bind： IP绑定修改为本机IP daemonize： 改为后台进程运行 启动 redis 服务./redis-server ../redis.conf 使用可以端连接到 redis./redis-cli -h 192.168.56.110 -p 6379 停止 redis 服务./redis-cli shutdown 4. 常用命令Key 相关keys 检索满足条件的键值对，支持正则表达式的通配符匹配，但要注意大数据量下的检索影响 redis 服务性能。123456## 获得一个符合匹配规则的键名列表，支持通配符keys [? / * [] ]## 判断key是否存在exists key## 获取key结构类型type key 字符类型最基本的数据结构，可以存储任意的字符类型的数据，单条数据最大可支持512M。12345678910111213141516171819202122## 设置key/value键值, 过期时间 EX=10s, PX=100msset key value EX 10 PX 100 ## 批量设置多个key/valuemset key value key1 value1## 只在key不存在时进行设置setnx key value## 获取key值get key## 获取多个key的值mget key key1## 对数字类型原子递增 1incr num## 原子递减 1decr num## 原子递增 10incrby num 10## 原子递减 10decrby num 10## 向指定的key追加字符串append key value## 获取key对应的value的长度strlen key 列表类型list 可以存储一个有序的字符列表，内部是使用双向链表实现的，可以用来实现分布式消息队列。命令以 l-开头。123456789101112## 左右两端添加数据(r|l)push key value value1 value2## 左右两端弹出数据，输出弹出的value(r|l)pop key## 获取列表长度llen key## 获取列表元素，start 开始索引， stop 结束索引(-1表示最右边)lrange key start stop## 从列表中移除N个value值的元素lrem key count value## 更新idx位置处的元素为valuelset key idx value 散列类型散列类型是 HashMap 数据结构，存储多个键值对，适合存储多属性对象，但不支持数据类型的嵌套。命令以 h- 开头。123456789101112## 设置对象field属性值hset key field valuehmset key field value field2 value2## 获取field属性值hget key fieldhmget key field field2## 获取所有field值hgetall key## 判断field属性是否存在hexists key field 集合类型集合类型 set 与列表不同，不能存在重复的数据，而且是无序存储。命令以 s- 开头。12345678910111213141516## 增加元素，如果value存在则忽略，返回成功加入集合的数量sadd key value value2## 删除元素srem key value## 获取所有元素smembers key## 获取集合长度scard key## 集合key1与key2的差集，列出key1中存在而在key2中不存在的元素sdiff key1 key2## 将差集存储在des中sdiffstore des key key2## 集合key1与key2的交集sinter key1 key2## 集合key1与key2的并集sunion key1 key2 有序集合类型有序集合是在集合set的基础上，增加了排序功能，增加了score表示元素的优先级。命令以 z- 开头。1234## 增加元素，优先级 scorezadd key score value## 列出集合，并且输出优先级zrange key start stop withscores 事务处理Redis 中支持事务，将多个命令加入到QUEUED中到最后一起提交或回滚，但有特例的情况无法回滚(命令运行时出错)。12345## 开启事务multi...## 提交事务exec 过期时间Redis 中可以对每个键值设置对应的过期时间。1234## 设置key过期时间为seconds秒expire key seconds## 获得key的过期时间,-1 未设置 -2已过期 ttl key 发布订阅(pub/sub)Redis 中支持消息的发布订阅模式，类型消息中间件的功能，但性能不高一般不推荐使用。1234## 发布消息到channel频道publish channel msg## 订阅channel频道的消息subscribe channel","tags":[{"name":"分布式","slug":"分布式","permalink":"https://joecnn.github.io/wiki-site/tags/分布式/"},{"name":"redis","slug":"redis","permalink":"https://joecnn.github.io/wiki-site/tags/redis/"}],"categories":[{"name":"技术开发","slug":"技术开发","permalink":"https://joecnn.github.io/wiki-site/categories/技术开发/"},{"name":"分布式","slug":"技术开发/分布式","permalink":"https://joecnn.github.io/wiki-site/categories/技术开发/分布式/"}]},{"title":"Kafka 实现原理","date":"2018-12-28T16:00:00.000Z","path":"wiki/技术开发/分布式/Kafka 实现原理/","text":"1. 消息可靠性机制消息发送可靠性消息发送到 broker 有三种确认方式 request.required.acks ： acks=0 producer 不会等待 broker 发送ack，既可能丢失也可能重发。 acks=1 当 leader 接收到消息后发送ack，可能重发。 acks=-1 当所有 follower 同步成功后发送ack，丢失消息可能性低。 消息存储可靠性每一条消息发送到broker中，会跟据partition规则存储到对应分区，如果规则设置合理可以实现消息均匀分布到不同分区，实现存储的水平扩展。高可靠性的保障来自另一个叫副本(replication)策略，通过设置(–replication-factor)参数设置。 2. 副本存储机制Kafka 在创建 topic 支持设置对应的副本个数 --replication-factor，会生成对应的分区副本数交叉存储在每个节点上。 副本存活条件 副本所有节点必须和zookeeper保持连接状态 副本的最后一条消息的offset和leader的最后一条消息的offset之间差值不能超过设定值replica.lag.max.messages 如何同步消息第一个启动的节点成功在zookeeper中注册信息成为leader对外提供服务，其它replica做为follower要定时同步数据。 HW(high watermark) ：表示所有follower已同步完成的offset位置 LEO(Log End Offset) ：表示leader节点当前消息的offset位置 consumer只能消费所有follower已同步完成的数据，即HW标注的位置。 如何均匀分布Kafka 为了更好的做到负载均衡，会尽量把所有的 partition 均匀分配到整个集群上，分配的算法： 把所有 broker(n) 和 partition(n) 排序 把第i个partition分配到 (i%n) 个broker上 把第 i 个 partition 的第 j 个 replica 分配到 ((i+j) % n) 个broker上 如何处理所有副本不工作情况在ISR中至少有一个follower工作时，Kafka可以确保消息不丢失，但如果某个分区所有备份都宕机了，采取以下措施： 等待ISR中任意一个follower活过来，并且选择它为leader 选择任意一个replica(不一定是ISR中的)活过来作为leader 这两种需要在可用性和一致性当中做一个选择。 3. Kafka 文件存储机制partition每个partition为一个目录，命名规则为 &lt;topic_name&gt;-&lt;partition_no&gt;，存储在 kafka_logs 目录下。 segmentKafka 为防止分区文件过大，又将分区拆分为 segment 存储，一个segment文件.index和.log两部分组成，即索引文件和数据文件。segment 文件由64位long数值命名，文件名即记录的最大 offset 值，查找时先通过定位 offset 落的范围，再进入文件查找。 日志保留Kafka 中无论是否消费了消息(只是移动了 offset)，都会一直保留这些消息，为了避免磁盘爆满，使用相应的保留策略(retention policy)，以实现周期性的删除陈旧消息： 根据消息保留时间，超过指定时间则删除 根据 topic 大小，超过阈值开始删除最旧消息 日志压缩Kafka 会定期将相同 Key 的消息进行合并，只保留最新的 Value 值。 4. Kafka 消息的消费原理旧版本的 Kafka 将 Consumer group 的消费进度记录在 zookeeper 中，导致对 zookeeper 频繁的写入而性能低下。新版本1.0+已修改为记录在对应 topic 目录下，默认创建了50个 __consumer_offset_topic 文件夹，将消费的进度 offset 保存在对应的文件中。 计算 consumer group 的哈希值 1Math.abs(\"group1\".hashCode() % 50) 根据哈希值找到对应的__consumer文件，查看消费进度 1sh kafka-simple-consumer-shell.sh --topic __consumer_offsets --partition 15 -broker-list 192.168.56.110:9092,192.168.56.120:9092,192.168.56.130:9092 --formatter kafka.coordinator.group.GroupMetadataManager\\$OffsetsMessageFormatter 5. Kafka 的消费者分区分配策略Kafka中存在 consumer group 的概念，也就是group.id一样的consumer属于一个consumer group，组内的所有消费者协调在一起来消费消费订阅主题的所有分区。当然每一个分区只能由同一个消费组内的consumer来消费，那么同一个consumer group里面的consumer是怎么去分配该消费哪个分区里的数据，这个就设计到了kafka内部分区分配策略（Partition Assignment Strategy）。在 Kafka 内部存在两种默认的分区分配策略：Range（默认） 和 RoundRobin。通过：partition.assignment.strategy指定 两种分区策略Range （默认策略）0 ，1 ，2 ，3 ，4，5，6，7，8，9c0 [0,3]c1 [4,6]c2 [7,9]10(partition num)/3(consumer num) =3 RoundRobin （轮询）0 ，1 ，2 ，3 ，4，5，6，7，8，9c0,c1,c2c0 [0,3,6,9]c1 [1,4,7]c2 [2,5,8]kafka 的key 为null， 是随机｛一个Metadata的同步周期内，默认是10分钟｝ 6. 高吞吐量原因 消息顺序存储，通过 offset 偏移量进行顺序读取，减少机械硬盘磁柱移动。 消息批量发送，在异步模式中允许进行批量发送消息，先将消息缓存到内存，再一次请求中批量发送出去，减少磁盘读写和网络传输。 batch.size 每批量发送的数据大小 linger.ms 批量发送的间隔时间 消息的零拷贝，使用 FileChannel.transferTo 直接将消息发送到 socket buffer 中，省略了将消息读取到内存的过程。","tags":[{"name":"分布式","slug":"分布式","permalink":"https://joecnn.github.io/wiki-site/tags/分布式/"},{"name":"MQ","slug":"MQ","permalink":"https://joecnn.github.io/wiki-site/tags/MQ/"}],"categories":[{"name":"技术开发","slug":"技术开发","permalink":"https://joecnn.github.io/wiki-site/categories/技术开发/"},{"name":"分布式","slug":"技术开发/分布式","permalink":"https://joecnn.github.io/wiki-site/categories/技术开发/分布式/"}]},{"title":"Kafka quickstart","date":"2018-12-28T16:00:00.000Z","path":"wiki/技术开发/分布式/Kafka quickstart/","text":"1. Kafka是什么? Apache Kafka® is a distributed streaming platform. Kafka是一个高性能、高吞吐量的分布式消息通信系统。常用于系统的日志收集分析、消息通信、用户行为分析、服务器指标监控、流式处理等。 2. Kafka集群安装 下载kafka二进制安装包, 并解压到 /usr/local目录下。 进入到config目录下修改server.properties配置文件： broker.id 在集群中的唯一编号 listeners 本机ip num.partitions 默认的topic分区数 zookeeper.connect 连接到zookeeper集群，可以设置根路径 ip:port/kafka 进入到bin目录下，以守护进程方式启动。1$ sh kafka-server-start.sh -daemon ../config/server.properties 3. Kafka基本操作 创建一个topic 1$ sh kafka-topics.sh --create --zookeeper 192.168.56.110:2181/kafka --replication-factor 1 --partitions 2 --topic first-topic 列出所有topic 1$ sh kafka-topics.sh --list --zookeeper 192.168.56.110:2181/kafka 生成者发送消息 1$ sh kafka-console-producer.sh --broker-list 192.168.56.110:9092 --topic first-topic 消费者接收消息 1$ sh kafka-console-consumer.sh --bootstrap-server 192.168.56.110:9092 --topic first-topic --from-beginning 查看消息日志 1$ sh kafka-run-class.sh kafka.tools.DumpLogSegments --files /tmp/kafka-logs/first-topic-0/00000000000000000000.log --print-data-log 4. Kafka中的概念Message消息是 Kafka 中最基本的数据单元，由 Key/Value 组成(byte[])，根据Key的哈希值路由到不同区间进行存储。Kafka 会对消息进行压缩和批量发送。 Topictopic 是用于存储消息的逻辑单元，可以看作一个消息集合，每个 topic 可以有多个生产者向其推送消息，也可以有任意多个消费者。 Partition每个 topic 可以划分多个分区，即 Kafka 存储消息的物理单元。Partition 是以文件的形式存储在 kafka-logs 目录下，命名规则是 &lt;topic_name&gt;-&lt;partition_id&gt;。 GroupKafka 中的逻辑分组，同一个组内的一条消息只被一个组员消费，并且共同维护 consumer offset。不同组内的消费者可以同时消费同一条消息，实现 pub/sub 模式。","tags":[{"name":"分布式","slug":"分布式","permalink":"https://joecnn.github.io/wiki-site/tags/分布式/"},{"name":"MQ","slug":"MQ","permalink":"https://joecnn.github.io/wiki-site/tags/MQ/"}],"categories":[{"name":"技术开发","slug":"技术开发","permalink":"https://joecnn.github.io/wiki-site/categories/技术开发/"},{"name":"分布式","slug":"技术开发/分布式","permalink":"https://joecnn.github.io/wiki-site/categories/技术开发/分布式/"}]},{"title":"ActiveMQ configuration","date":"2018-12-26T16:00:00.000Z","path":"wiki/技术开发/分布式/ActiveMQ configuration/","text":"ActiveMQ使用过程中涉及到的一些配置信息。 1. 传输协议配置支持 TCP, UDP, NIO, SSL, HTTP(S), VM 等协议.1234&lt;transportConnectors&gt; &lt;!-- DOS protection, limit concurrent connections to 1000 and frame size to 100MB --&gt; &lt;transportConnector name=\"nio\" uri=\"nio://0.0.0.0:61618?maximumConnections=1000&amp;amp;wireFormat.maxFrameSize=104857600\" /&gt;&lt;/transportConnectors&gt; 2. 持久化策略配置支持的持久化方式有 kahaDB, AMQ, JDBC, Memory. 使用 AMQ 文件存储123&lt;persistenceAdapter&gt; &lt;amqPersistenceAdapter directory=\"$&#123;activemq.data&#125;/amq\" maxFileLength=\"32m\"/&gt; &lt;/persistenceAdapter&gt; 使用 JDBC 存储使用jdbc数据库存储的方式，需要连接到对应的数据源，要在../lib下加入连接包。123456789101112&lt;!-- jdbc存储策略 --&gt;&lt;persistenceAdapter&gt; &lt;jdbcPersistenceAdapter dataSource=\"#mysqlDataSource\" createTablesOnStartup=\"true\" /&gt;&lt;/persistenceAdapter&gt;&lt;!-- 数据源配置 --&gt;&lt;bean id=\"mysqlDataSource\" class=\"org.apache.commons.dbcp.BasicDataSource\" destroy-method=\"close\"&gt; &lt;property name=\"driverClassName\" value=\"com.mysql.jdbc.Driver\" /&gt; &lt;property name=\"url\" value=\"jdbc:mysql://192.168.56.104:3306/practice_dev\" /&gt; &lt;property name=\"username\" value=\"root\" /&gt; &lt;property name=\"password\" value=\"123456\" /&gt; &lt;/bean&gt; 3. 集群配置配置多态activemq服务进行联通，提高消息服务的性能。1234&lt;!-- broker 添加此配置 --&gt;&lt;networkConnectors&gt; &lt;networkConnector uri=\"static://(tcp://192.168.56.110:61616,tcp://192.168.56.120:61616)\" /&gt;&lt;/networkConnectors&gt; 配置集群后，当使用某一个节点消费时，会将另外节点的数据转移到此节点上，导致在原节点无法再消费。此时需要配置==消息回流==123456&lt;!-- 消息回流支持，添加在 policyEntries下--&gt;&lt;policyEntry queue=\"&gt;\" enableAudit=\"false\"&gt; &lt;networkBridgeFilterFactory&gt; &lt;conditionalNetworkBridgeFilterFactory replayWhenNoConsumers=\"true\" /&gt; &lt;/networkBridgeFilterFactory&gt;&lt;/policyEntry&gt; 4. zk+activemq高可用配置使用ZK进行master/slaver选举管理，在ZK中维护了临时有序节点，最先启动的先获得master。 directory： levelDB数据文件存储的位置 replicas：计算公式（replicas/2）+1 ， 当replicas的值为2的时候， 最终的结果是2. 表示集群中至少2台是启动时才能提供服务 bind: 用来负责slave和master的数据同步的端口和ip zkAddress： 表示zk的服务端地址 hostname：本机ip1234567&lt;persistenceAdapter&gt; &lt;replicatedLevelDB directory=\"$&#123;activemq.data&#125;/levelDB\" replicas=\"2\" bind=\"tcp://0.0.0.0:61615\" zkAddress=\"192.168.56.110:2181\" hostname=\"192.168.56.110\" zkPath=\"/activemq/leveldb\" /&gt;&lt;/persistenceAdapter&gt; 5. hawtio 监控服务 拷贝hawtio.war包到webapps目录下. 添加jetty容器映射rewriteHandler: 12345&lt;bean class=\"org.eclipse.jetty.webapp.WebAppContext\"&gt; &lt;property name=\"contextPath\" value=\"/hawtio\" /&gt; &lt;property name=\"war\" value=\"$&#123;activemq.home&#125;/webapps/hawtio.war\" /&gt; &lt;property name=\"logUrlOnStart\" value=\"true\" /&gt; &lt;/bean&gt; 修改 bin/env 文件添加参数 需要注意的是-Dhawtio的三个设定必须放在ACTIVEMQ_OPTS设置的最前面(在内存参数设置之后),否则会出现验证无法通过的错误(另外,ACTIVEMQ_OPTS的设置语句不要回车换行) 1-Dhawtio.realm=activemq -Dhawtio.role=admins -Dhawtio.rolePrincipalClasses=org.apache.activemq.jaas.GroupPrincipal","tags":[{"name":"分布式","slug":"分布式","permalink":"https://joecnn.github.io/wiki-site/tags/分布式/"},{"name":"MQ","slug":"MQ","permalink":"https://joecnn.github.io/wiki-site/tags/MQ/"}],"categories":[{"name":"技术开发","slug":"技术开发","permalink":"https://joecnn.github.io/wiki-site/categories/技术开发/"},{"name":"分布式","slug":"技术开发/分布式","permalink":"https://joecnn.github.io/wiki-site/categories/技术开发/分布式/"}]},{"title":"事务的4种隔离级别","date":"2018-12-23T11:20:00.000Z","path":"wiki/技术开发/存储/事务的4种隔离级别/","text":"数据库事务的隔离级别有4种，由低到高分别为Read uncommitted 、Read committed 、Repeatable read 、Serializable 。而且，在事务的==并发操作==中可能会出现脏读，不可重复读，幻读。下面通过事例一一阐述它们的概念与联系。 Read uncommitted读未提交，顾名思义，就是一个事务可以读取另一个未提交事务的数据。 事例：老板要给程序员发工资，程序员的工资是3.6万/月。但是发工资时老板不小心按错了数字，按成3.9万/月，该钱已经打到程序员的户口，但是事务还没有提交，就在这时，程序员去查看自己这个月的工资，发现比往常多了3千元，以为涨工资了非常高兴。但是老板及时发现了不对，马上回滚差点就提交了的事务，将数字改成3.6万再提交。 分析：实际程序员这个月的工资还是3.6万，但是程序员看到的是3.9万。他看到的是老板还没提交事务时的数据。这就是==脏读==。 那怎么解决脏读呢？Read committed！读提交，能解决脏读问题。 Read committed读提交，顾名思义，就是一个事务要等另一个事务提交后才能读取数据。 事例：程序员拿着信用卡去享受生活（卡里当然是只有3.6万），当他埋单时（程序员事务开启），收费系统事先检测到他的卡里有3.6万，就在这个时候！！程序员的妻子要把钱全部转出充当家用，并提交。当收费系统准备扣款时，再检测卡里的金额，发现已经没钱了（==第二次检测金额当然要等待妻子转出金额事务提交完==）。程序员就会很郁闷，明明卡里是有钱的… 分析：这就是读提交，若有事务对数据进行更新（UPDATE）操作时，读操作事务要等待这个更新操作事务提交后才能读取数据，可以解决脏读问题。但在这个事例中，出现了==一个事务范围内两个相同的查询却返回了不同数据==，这就是==不可重复读==。 那怎么解决可能的不可重复读问题？Repeatable read ！ Repeatable read重复读，就是在开始读取数据（事务开启）时，不再允许修改操作 事例：程序员拿着信用卡去享受生活（卡里当然是只有3.6万），当他埋单时（==事务开启，不允许其他事务的UPDATE修改操作==），收费系统事先检测到他的卡里有3.6万。这个时候他的妻子不能转出金额了。接下来收费系统就可以扣款了。 分析：重复读可以解决不可重复读问题。写到这里，应该明白的一点就是，==不可重复读对应的是修改，即UPDATE操作==。但是可能还会有幻读问题。因为==幻读==问题对应的是插入INSERT操作，而不是UPDATE操作。 什么时候会出现幻读？事例：程序员某一天去消费，花了2千元，然后他的妻子去查看他今天的消费记录（全表扫描FTS，妻子事务开启），看到确实是花了2千元，就在这个时候，程序员花了1万买了一部电脑，即新增==INSERT==了一条消费记录，并提交。当妻子打印程序员的消费记录清单时（妻子事务提交），发现花了1.2万元，似乎出现了幻觉，这就是幻读。 那怎么解决幻读问题？Serializable！ Serializable 序列化Serializable 是最高的事务隔离级别，在该级别下，事务==串行化顺序执行==，可以避免脏读、不可重复读与幻读。但是这种事务隔离级别效率低下，比较耗数据库性能，一般不使用。 值得一提的是：大多数数据库默认的事务隔离级别是Read committed，比如Sql Server , Oracle。Mysql的默认隔离级别是Repeatable read。","tags":[],"categories":[{"name":"技术开发","slug":"技术开发","permalink":"https://joecnn.github.io/wiki-site/categories/技术开发/"},{"name":"存储","slug":"技术开发/存储","permalink":"https://joecnn.github.io/wiki-site/categories/技术开发/存储/"}]},{"title":"zookeeper 应用篇","date":"2018-12-17T16:00:00.000Z","path":"wiki/技术开发/分布式/zookeeper 应用篇/","text":"实际应用zookeeper 实际应用非常的广泛，只要涉及到分布式系统中的问题，都可以尝试使用 zookeeper 解决。 但 zookeeper 不是内存数据库，存储的内容应该都是比较简短的配置信息，zookeeper 的常用应用如下： 配置管理中心 软负载均衡 分布式锁 master选举 分布式队列 1. 配置管理中心在分布式系统中，多个服务运行在不同的服务器上，这时候再使用 application.properties 文件进行配置管理，将会非常的繁琐和易出错。使用 zookeeper 做为统一的配置中心，可以实现配置信息动态设置，多服务节点切换等功能。 实现思路 开源配置中心实现：disconf 创建统一个节点 /configure 作为配置信息的根节点。 客户端启动时拉取(pull)所有的子节点，载入到内存进行缓存。 启动后的客户端，对关注的配置节点进行 watch 获取到后续动态更新的配置信息(push)。 可以按module划分配置信息节点树。 2. 分布式锁在分布式系统中，多个服务对同一资源进行争抢时要用到锁，防止因为并发操作导致数据出现的不一致行为。使用 zookeeper 可以优雅的思想分布式锁，而且性能堪比 redis 实现。 实现思路 开源分布式锁实现：curator lock 创建统一节点 /locks 作为业务锁的根节点。 当服务执行时，均在 locks 下创建临时有序节点。 当前序号最小的节点获得锁，序号大的监听上一个节点的删除事件 当捕获到删除事件，或 session 超时(临时节点特性，连接断开删除节点)时，后一个节点获得锁。 可以扩展做读写锁，分类锁，业务模块锁。 PS：如果使用创建同一节点的方式，会产生羊群效应，因为过多的服务请求涌入而压垮 zookeeper 集群。 3. master 选举在分布式系统中，为提高可用性通常可以使用冷备或热备的形式进行冗余，master-slaver模式下其实只有一个节点处理事务，而当 master 节点宕机后，热备的 slaver 自动顶替 master 节点进行处理。 实现思路 开源 master选举实现：curator selector 在服务器启动时，均到 zookeeper 参数创建临时节点 /master。 创建成功的服务器成为 master, 其它服务器进行事件监听。 当事件触发后，重新进行选举(热备自动切换)。","tags":[{"name":"分布式","slug":"分布式","permalink":"https://joecnn.github.io/wiki-site/tags/分布式/"}],"categories":[{"name":"技术开发","slug":"技术开发","permalink":"https://joecnn.github.io/wiki-site/categories/技术开发/"},{"name":"分布式","slug":"技术开发/分布式","permalink":"https://joecnn.github.io/wiki-site/categories/技术开发/分布式/"}]},{"title":"How To Open A Port In CentOS 7 With Firewalld","date":"2018-12-16T16:00:00.000Z","path":"wiki/技术开发/linux/How To Open A Port In CentOS 7 With Firewalld/","text":"This tutorial will walk you through opening a port in the default firewall in CentOS 7, firewalld. You will see that while we can manually open a specific port, it is often easier and beneficial to allow based on predefined services instead. Open Specific PortOpening a port in firewalld is fairly straightforward, in the below example we allow traffic in from any source IP address to TCP port 100. First we modify the persistent configuration, then we reload firewall-cmd to load this change into the running configuration.1234[root@centos7 ~]# firewall-cmd --permanent --add-port=100/tcpsuccess[root@centos7 ~]# firewall-cmd --reloadsuccess If the –permanent flag is not specified, this will only change the running configuration but will not be saved. We can check the ports that are opened in the current default zone with ‘–list-ports’.12[root@centos7 ~]# firewall-cmd --list-ports100/tcp As expected we see that TCP port 100 is open. Should we wish to remove a port, we can use ‘–remove-port=’ instead. We can also open a range of ports in the same way.12[root@centos7 ~]# firewall-cmd --permanent --add-port=200-300/tcpsuccess Open Predefined ServiceRather than manually specifying a port number to allow through the firewall, we can make use of a bunch of predefined services which may be easier. For example instead of opening TCP port 80, we can use the ‘http’ service. 1234[root@centos7 ~]# firewall-cmd --permanent --add-service=httpsuccess[root@centos7 ~]# firewall-cmd --reloadsuccess Now if we list the services that are accepted through the firewall, we will see http listed along with ssh and dhcpv6-client, which are allowed through by default.12[root@centos7 ~]# firewall-cmd --list-servicesdhcpv6-client http ssh This is a predefined service and can be found as an XML file in the /usr/lib/firewalld/services/ directory. Here’s what the http service we just used looks like. 1234567[root@centos7 ~]# cat /usr/lib/firewalld/services/http.xml&lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;&lt;service&gt; &lt;short&gt;WWW (HTTP)&lt;/short&gt; &lt;description&gt;HTTP is the protocol used to serve Web pages. If you plan to make your Web server publicly available, enable this option. This option is not required for viewing pages locally or developing Web pages.&lt;/description&gt; &lt;port protocol=\"tcp\" port=\"80\"/&gt;&lt;/service&gt; We can create custom services by copying one of these into the /etc/firewalld/services/ directory and then customizing it. The services in the /usr/lib/firewalld/services/ directory should NOT be modified, changes should be copied into /etc/firewalld/services/ followed by a reload of firewall-cmd to pick up the changes. Services Or Manual Ports?Why would we want to use services if we can just specify the port? Modules can be specified in a service, for example samba.xml loads the module “nf_conntrack_netbios_ns” for us when it’s enabled, along with four different ports which is a lot easier than doing all of this ourselves as we don’t need to memorize all of the ports required for a service. Still not a fan of firewalld? Don’t worry, you can always install ifconfig in CentOS 7 instead, however note that this is considered deprecated. SummaryWe have seen that the firewall in CentOS 7 can be modified to open a specific port, or more preferably we can open it to a service. While these basic examples demonstrate opening a port to any source, this is usually not desirable. We can further filter based on source traffic with firewalld rich rules.","tags":[{"name":"linux","slug":"linux","permalink":"https://joecnn.github.io/wiki-site/tags/linux/"}],"categories":[{"name":"技术开发","slug":"技术开发","permalink":"https://joecnn.github.io/wiki-site/categories/技术开发/"},{"name":"linux","slug":"技术开发/linux","permalink":"https://joecnn.github.io/wiki-site/categories/技术开发/linux/"}]},{"title":"Zookeeper quickstart","date":"2018-12-15T16:00:00.000Z","path":"wiki/技术开发/分布式/Zookeeper quickstart/","text":"1. zookeeper 是什么？ ZooKeeper is a centralized service for maintaining configuration information, naming, providing distributed synchronization, and providing group services. zookeeper 是Apache基金会的一个项目，它为大型分布式计算提供开源的分布式配置服务、同步服务和命名注册，是分布式协调服务，目标是解决分布式数据的一致性问题。 2. zookeeper 能做什么？数据的发布/订阅(配置中心 disconf)、 负载均衡(dubbo)、唯一ID生成器、统一命名服务、master选举(kafka, hadoop, hbase)、分布式队列、分布式锁…… 3. zookeeper 的特性 zookeeper 用来做的任务都是由他的特性决定的，理解了他的特性就可以根据实际的场景选择具体的方案。 顺序一致性：从客户端发起的事务请求，严格按照顺序被应用到zookeeper中 原子性：所有事务请求在集群上应用情况是一致的，要么都成功，要么都失败。 可靠性：一旦服务器应用了某个事务数据，那么这个数据一定是同步并且保留下来的。 实时性：一旦某个事务被成功应用，客户端能立即读取到最新数据状态(zookeeper 仅仅保证一定时间内的近实时性)。 4. 安装 zookeeper4.1 单机环境安装 下载安装包 zookeeper-3.4.12.tar.gz 解压安装包tar -zxvf zookeeper-3.4.12.tar.gz -C /usr/local/ 创建配置文件cd conf/ &amp;&amp; cp zoo_sample.cfg zoo.cfg 启动 zookeepercd bin/ &amp;&amp; sh zkServer.sh start 使用客户端连接到 zookeeper 进行操作sh zkCli.sh -server ip:port 单机环境下启动 zookeeper 状态为 standalone 仅用于测试或学习。 4.2 集群环境安装 集群环境下至少需要2N+1台服务器进行安装 修改配置文件，添加主机列表 123456……# ip:port:port # ip地址 : 用于节点通信的端口 : 用于选举的端口 [:observer]server.1 = 192.168.56.110:2888:3888server.2 = 192.168.56.120:2888:3888server.3 = 192.168.56.130:2888:3888 添加 myid 文件在配置文件的 dataDir 目录下创建 myid 文件，文件就一行数据内容是每台机器对应的server.ID的数字。 启动 zookeeper 4.3 zookeeper 节点类型 leader : 领导节点，主要接收、分发请求，发起事务处理投票，并给 follower 节点同步数据。 follower : 随从节点，处理读请求，转发事务请求，并保持和 leader 节点同步和事务的投票选举。 observer : 监控节点，处理读请求，保持和 leader 节点同步但不进行投票选举。 5. zookeeper 客户端命令使用12345678910111213141516# 列出路径下的所有节点ls / # 创建节点# -s 临时节点 -e 有序节点 acl 权限create [-s] [-e] path data acl# 获取节点信息get path# 更新节点内容# version 类似数据库乐观锁的实现set path data [version]# 删除节点delete path [version]","tags":[{"name":"分布式","slug":"分布式","permalink":"https://joecnn.github.io/wiki-site/tags/分布式/"}],"categories":[{"name":"技术开发","slug":"技术开发","permalink":"https://joecnn.github.io/wiki-site/categories/技术开发/"},{"name":"分布式","slug":"技术开发/分布式","permalink":"https://joecnn.github.io/wiki-site/categories/技术开发/分布式/"}]},{"title":"Jenkins开机启动","date":"2018-10-18T12:30:00.000Z","path":"wiki/技术开发/devops/Jenkins开机启动/","text":"使用 jenkins 最简单的方式即使用 war 包进行启动，war 包中带了 jetty 服务，可以直接 java -jar jenkins.war 启动使用。但每次都使用命令相当繁琐，本编即介绍如何将此步骤设置于开机启动。 1. 环境准备 Linux CentOs 7.3 下载 Jre 1.8.0下载 Jenkins 2.138.2下载 2. 编写 linux 开机自运行脚本 jenkins.sh 将该脚本加入chkconfig启动项目中，开机时运行。 JENKINS_ROOT: jenkins软件目录 JENKINS_HOME: jenkins主目录 12345678910111213141516171819202122232425262728293031323334353637#!/bin/sh#chkconfig: 2345 80 90#description:开机启动jenkins服务JENKINS_ROOT=/usr/local/jenkinsJENKINSFILENAME=jenkins.war#停止方法stop()&#123; echo \"Stoping $JENKINSFILENAME \" ps -ef|grep $JENKINSFILENAME |awk '&#123;print $2&#125;'|while read pid do kill -9 $pid echo \" $pid kill\" done&#125;case \"$1\" instart) echo \"Starting $JENKINSFILENAME \" nohup $JENKINS_ROOT/start_jenkins.sh &gt;&gt; $JENKINS_ROOT/jenkins.log 2&gt;&amp;1 &amp; ;;stop) stop ;;restart) stop start ;;status) ps -ef|grep $JENKINSFILENAME ;;*) printf 'Usage: %s &#123;start|stop|restart|status&#125;\\n' \"$prog\" exit 1 ;;esac 3. 编写启动 war 包命令 start_jenkins.sh 启动war包的命令，由于在启动时需要使用java命令，所以在脚本中加入了java的bin路径。 12345#!/bin/bashJENKINS_ROOT=/usr/local/jenkinsexport JENKINS_HOME=$JENKINS_ROOT/homeJAVA_HOME=/usr/local/java/jre1.8.0_151 PATH=$PATH:$JAVA_HOME/binjava -jar $JENKINS_ROOT/jenkins.war --httpPort=8080 4. 加入 chkconifg 启动项目123456789# 赋予执行权限chmod +x /usr/local/jenkins/jenkins.sh# 创建软链接到 init.d 目录ln -s /usr/local/jenkins/jenkins.sh /etc/rc.d/init.d/jenkins# 添加到 chkconfigchkconfig --add jenkinschkconfig --level 345 jenkins on 5. 启动jenkins服务1/etc/rc.d/init.d/jenkins start 到此已经可以在启动服务器时自动运行jenkins了，端口占用8080.","tags":[{"name":"jenkins","slug":"jenkins","permalink":"https://joecnn.github.io/wiki-site/tags/jenkins/"}],"categories":[{"name":"技术开发","slug":"技术开发","permalink":"https://joecnn.github.io/wiki-site/categories/技术开发/"},{"name":"devops","slug":"技术开发/devops","permalink":"https://joecnn.github.io/wiki-site/categories/技术开发/devops/"}]},{"title":"添加开机运行脚本","date":"2018-10-17T16:00:00.000Z","path":"wiki/技术开发/linux/添加开机运行脚本/","text":"Linux 设置开机自动启动的方式有好多种，这里介绍一种通过 chkconfig 命令添加脚本为开机启动的方法。 1. 编写脚本 autostart.sh123456#!/bin/sh#chkconfig: 2345 80 90#description: 开机自动运行脚本# 开启redis服务 端口为6379/usr/local/service/redis-2.8.3/src/redis-server --port 6379 &amp; 脚本第二行表示在 2/3/4/5运行级别启动，启动序号80，关闭序号90 2. 赋予脚本执行权限1chmod +x /usr/local/service/redis-2.8.3/autostart.sh 3. 创建软链接到 init.d 目录下1ln -s /usr/local/service/redis-2.8.3/autostart.sh /etc/rc.d/init.d/redis 4. 添加到开机启动项中12chkconfig -add redischkconfig redis on 到这里已经将脚本添加到开机启动项中，重启服务器即可看到redis服务已经启动了。","tags":[{"name":"linux","slug":"linux","permalink":"https://joecnn.github.io/wiki-site/tags/linux/"}],"categories":[{"name":"技术开发","slug":"技术开发","permalink":"https://joecnn.github.io/wiki-site/categories/技术开发/"},{"name":"linux","slug":"技术开发/linux","permalink":"https://joecnn.github.io/wiki-site/categories/技术开发/linux/"}]},{"title":"Jenkins持续编译","date":"2018-10-17T13:00:00.000Z","path":"wiki/技术开发/devops/Jenkins持续编译/","text":"Jenkins 是一个开源自动化服务器，可用于自动化各种任务，如构建、测试和部署软件，本文档是结合Jenkins，Java，Maven，Github实现持续自动化编译。 1. 思路&amp;流程 安装 Java、Maven、Git、Jenkins 环境 配置 Jenkins 拉取 Github 项目 编译、单元测试 Maven 项目形成 war 包 2. 环境准备 由于 Maven 需要 jdk 支持，所以需要先配置 jdk 环境，再配置 maven 环境。 准备可联网的 Linux Centos 7.3 服务器 下载 Jdk1.8.0 并设置环境变量 下载 Maven3.3 并设置环境变量 安装 Git 3. 安装 Jenkins 可以设置 JENKINS_HOME 环境变量，改变 jenkins 启动生成文件存放位置.其它安装方式参考：Jenkins安装 首先从 Jenkins官方网站 下载最新的 war 包，只需运行命令： 1java -jar jenkins.war --httpPort=8080 Jenkins 服务就启动成功了，它的 war 包自带了 jetty 服务器，剩下的工作可以在浏览器内完成。 4. 配置 Jenkins首次进入 Jenkins 时，出于安全考虑， Jenkins 会自动生成一个随机口令，粘帖口令进入安装界面。进入 Jenkins 后选择 “Install suggested plugins“ 安装推荐插件，Jenkins 就自动配置好了 Maven、git 等常用插件。 在开始使用 Jenkins 创建项目前，需要在”系统管理“-&gt;”全局工具配置“中添加 JDK、Maven 设置：到此 Maven 项目的 Jenkins 已配置完成，下面开始创建构建任务。 5. 构建Maven项目在 Jenkins 首页选择”New 任务“，输入名字，选择”构建一个自由风格的软件项目“： 在配置页面中，”Source Code Management“ 选择 Git，填入地址，默认使用 mater 分支，如果为私人项目需要口令，在 Credentials 中添加用户名/口令： 在 “Build Triggers“ 中选择 “轮询 SCM“ 表示定时检查版本库，发现有新的提交就触发构建： 说明1：Triggerbuilds remotely(webhooks)这个选项就是配合 git 仓库的钩子功能实现代码 PUSH 后 Jenkins 收到通知自动触发构建项目的动作说明2：轮询 SCM定时检查源码变更，如果有更新就克隆下最新 code 下来，然后执行构建动作 在”Build“中可以添加编译命令，Maven默认的Root POM是pom.xml，如果pom.xml不在根目录下，则需要填入子目录： 说明1：选择之前添加的 maven 环境说明2：填入需要执行的 mvn 命令说明3：pom 不在根目录下，填入子目录 wxsell/pom.xml 保存后就可以”立即构建“，可以在”Console Output“中看到控制台详细输出： 6. 总结到此已配置了 Jenkins 自动编译任务，当 Github 上项目有变更时，会自动拉取项目进行编译，排除了可能不同机器上编译环境不同导致的影响。在完成持续编译后，可以结合 Jenkins 的编译后动作进行自动部署，实现持续部署功能。在下篇笔记中将会记录如何实现持续部署。","tags":[{"name":"jenkins","slug":"jenkins","permalink":"https://joecnn.github.io/wiki-site/tags/jenkins/"}],"categories":[{"name":"技术开发","slug":"技术开发","permalink":"https://joecnn.github.io/wiki-site/categories/技术开发/"},{"name":"devops","slug":"技术开发/devops","permalink":"https://joecnn.github.io/wiki-site/categories/技术开发/devops/"}]},{"title":"VirtualBox共享文件","date":"2018-10-15T16:00:00.000Z","path":"wiki/技术开发/工具/VirtualBox 共享文件/","text":"VirtualBox 与 Linux 虚拟机 可以通过 VBoxGuestAdditions 插件进行文件共享。 1. 设置共享文件夹在 VirtualBox 的共享文件夹设置中添加设置。 2. 安装 VBoxGuestAdditions 插件 需要在 linux 虚拟机中安装 将 $VirtualBox\\VBoxGuestAdditions.iso 装载到光驱 挂载 cdrom： 1$ mkdir /media/cdrom &amp;&amp; mount /dev/sr0 /media/cdrom 安装插件： 1$ cd /media/cdrom &amp;&amp; ./VBoxLinuxAdditions.run 3. 验证挂载12$ mount -t vboxsf share ~/share$ df | grep &apos;share&apos; 4. 问题与解决 mount: unknown filesystem type ‘vboxsf’. 未成功安装 VBoxGuestAdditions 插件 VBoxGuestAdditions 安装失败. 查看 /var/log/vbxadd-install.log 错误信息，一般为依赖的包未安装$ yum install gcc kernel-devel kernel-headers dkms make bzip2 ERROR: Kernel configuration is invalid. 此错误是编译的 VBoxGuestAdditions 版本太老，新的 kernel 不必指定include/linux/autoconf.h，更新virtualbox即可 Please install the Linux kernel “header” files matching the current kernel for adding new hardware support to the system. 此问题为默认安装的 kernel-devel 包与当前内核版本不同，重新安装对应包即可$ sudo yum install -y &quot;kernel-devel-$(uname -r)&quot;","tags":[],"categories":[{"name":"技术开发","slug":"技术开发","permalink":"https://joecnn.github.io/wiki-site/categories/技术开发/"},{"name":"工具","slug":"技术开发/工具","permalink":"https://joecnn.github.io/wiki-site/categories/技术开发/工具/"}]},{"title":"Docker镜像仓库加速","date":"2018-08-05T06:00:00.000Z","path":"wiki/技术开发/devops/docker/docker 镜像加速/","text":"docker 在默认安装后，当需要下载镜像时，通过命令docker pull user/image 拉取镜像都是访问默认的 docker hub 上的镜像，在国内网络环境下，下载一个镜像需要很长的时间，可以考虑使用 Registry Mirror 配置国内的镜像仓库。 使用由阿里云提供的 Docker 镜像仓库进行加速。 1. 登录阿里云 阿里云 Docker 镜像仓库 开启 Docker Hub 镜像站点 2. Windows 使用 Docker 加速 创建一台 docker machine 同时配置 docker 加速器 1docker-machine create --engine-registry-mirror=https://6bybmq21.mirror.aliyuncs.com -d virtualbox default 对于已经创建的 docker machine 实例，更换镜像源方法如下i. 在 window 命令执行 docker-machine ssh [machine-name] 进入 VM bashii. sudo vi /var/lib/boot2docker/profileiii. 在--label provider=virtualbox的添加一行 --registry-mirror https://xxx.mirror.aliyuncs.comiiii. 重启 docker 服务：sudo /etc/init.d/docker restart 或重启 VM ： docker-machine restart docker for windows 设置 Daemon Registry mirrors 查看 mirror 配置 123docker-machine env defaulteval &quot;$(docker-machine env default)&quot;docker info 3. CentOs 使用镜像加速 安装/升级 Docker 客户端 1curl -sSL http://acs-public-mirror.oss-cn-hangzhou.aliyuncs.com/docker-engine/internet | sh - 修改 daemon 配置文件 /etc/docker/daemon.json 来使用加速 12345678sudo mkdir -p /etc/dockersudo tee /etc/docker/daemon.json &lt;&lt;-&apos;EOF&apos;&#123; &quot;registry-mirrors&quot;: [&quot;https://6bybmq21.mirror.aliyuncs.com&quot;]&#125;EOFsudo systemctl daemon-reloadsudo systemctl restart docker","tags":[{"name":"docker","slug":"docker","permalink":"https://joecnn.github.io/wiki-site/tags/docker/"}],"categories":[{"name":"技术开发","slug":"技术开发","permalink":"https://joecnn.github.io/wiki-site/categories/技术开发/"},{"name":"devops","slug":"技术开发/devops","permalink":"https://joecnn.github.io/wiki-site/categories/技术开发/devops/"},{"name":"docker","slug":"技术开发/devops/docker","permalink":"https://joecnn.github.io/wiki-site/categories/技术开发/devops/docker/"}]},{"title":"Docker Start","date":"2018-08-05T04:30:00.000Z","path":"wiki/技术开发/devops/docker/docker start/","text":"1. Window10 下安装 Window10 下载最新的 Docker for windows 安装包进行安装 Windows10 下可以切换 Docker Platform 为 Linux 或者 Window，下载的镜像需要对应的平台支持。 Window10 下自带了虚拟器，需要开启 Hyper-V，在 控制面板-程序-打开Windows功能 中开启 Windows10 下的 Docker 使用虚拟机 IP 即为本机 127.0.0.1 ，可以在 CMD、PowerShell 中直接进行操作。 2. Windows7 下安装 Windows7 下在最新的 DockerToolBox 安装包进行安装DockerToolBox 会自动安装 Oracle Virtual Box 虚拟机，需要依托虚拟机进行启动。 2.1 创建 docker machine 安装完成后会附带 docker-machine.exe 使用此工具可以安装 docker 环境 查询 docker machine 1docker-machine ls 创建 default machine 使用 virtualbox 驱动模式，会自动下载 docker2boot.iso 镜像进行创建 virtualbox 虚拟机 1docker-machine create -d virtalbox default 查看 docker 信息 1docker-machine env default 管理 docker machine 1docker-machine -h 2.2 使用 putty 在 Windows 命令进入 Docker 后，shell 不能复制，操作不方便，因此使用支持 SSH 的工具来连接 docker 虚拟机。 查询 docker mechine ip： 在 docker machine env 中的 DOCKER_HOST 1docker-machine env default 使用 Putty 连接到 docker 终端： 默认用户名：docker 密码： tcuser PS：在 DockerToolBox 下自动安装的是 VirtualBox5.1，但是在启动 docker default mechine 时一直出错（VERR_SUPDRV_HARDENIGN_EVIL_HANDLE），经过查询是由于该版本的 vboxdrv 核心驱动请求过大的访问权限。。最后安装旧版的 VirtualBox4.3.12 后成功解决。 2.3 docker virtualbox 管理由于使用 docker-machine 创建的虚拟机默认路径为 /user/.docker/machine/ 下，可能在以后容器创建过多时占用系统盘资源。将 virtualbox 虚拟硬盘移动到其它盘。 复制 default vmdk 到指定路径 在指定盘进行复制一份 vmdk 为 default 虚拟机添加新的虚拟硬盘 添加新的虚拟硬盘后，删除原来的 disk.vmdk 文件即可 修改完成后，重启 docker machine 即可 参考资料： 白皮书：https://www.gitbook.com/book/yeasy/docker_practice/details 官方：https://www.docker.com/ 镜像库：https://hub.docker.com/","tags":[{"name":"docker","slug":"docker","permalink":"https://joecnn.github.io/wiki-site/tags/docker/"}],"categories":[{"name":"技术开发","slug":"技术开发","permalink":"https://joecnn.github.io/wiki-site/categories/技术开发/"},{"name":"devops","slug":"技术开发/devops","permalink":"https://joecnn.github.io/wiki-site/categories/技术开发/devops/"},{"name":"docker","slug":"技术开发/devops/docker","permalink":"https://joecnn.github.io/wiki-site/categories/技术开发/devops/docker/"}]},{"title":"《刻意练习》读书笔记","date":"2018-05-11T16:00:00.000Z","path":"wiki/阅读学习/刻意练习/","text":"格拉德威尔1万小时定律：人们眼中的天才之所有卓越非凡，并非天资超人一等，而是付出了持续不断的努力。只要经过1万小时的锤炼，任何人都能从平凡变成超凡。 从不存在1万小时定律，它仅仅是畅销书作家多心理科学研究的一次不太严谨的演绎而已。 首先，不同专业的技能习得时间与练习时间并不存在一个1万小时的最低阈值。其次，成功与练习时间并不完全成正比，一部分取决于天赋也取决于练习的方法。 刻意练习的本质是长时记忆，那些卓越的专家能够将工作记忆与长时记忆对接起来，在进行专业活动时，能够调用更大容量的工作记忆。例如专家升级SSD为虚拟内存，新手还在使用小内存。 刻意练习的任务难度要适中，能收到反馈，有足够的次数重复练习，学习者能够纠正自己的错误。 长时记忆的培养要点： 赋予意义、精细编码：专家们能非常快地明白自己领域的单词与术语，在存储信息时，可以有意识的采取原认知的各项加工策略。 提取结构或模式：往往需要将专业领域的知识、提取结构或者模式以更好的方式存储。比如，开发者善用设计模式。 加快速度、增加连接：通过大量重复的刻意练习，专家在编码与提取过程比新手都快很多，增加了长时记忆与工作记忆之间的各种通路。 人们的学习受到情境的制约或促进。你要学习的 东西将实际应用在什么情境中，那么你就应该在什么样的情境中学习这些东西。比如，你要学习编程，就应该在GitHub里学习。 从“情境学习”出发，当一名“认知学徒”，逐步成为专家： 找到学习共同体 隐形知识显性化，一般被称为策略知识 模仿榜样 培养多样性：在多种情境中实践，扩展应用范围 “绝对音感”并不是只有少数人才拥有的天赋，而是一种只要经过适度的接触和训练，几乎人人都可以培养和发展的能力。 “天才”是训练的产物 “天才”更懂得利用大脑的适应力 一、有目的的练习有目的的练习VS天真的练习 所谓“天真的练习”基本上是反复地做某件事情，并指望只靠那种反复，就能提高表现和水平。 有目的的练习的四个特点： 有目的的练习具有定义明确的特点目标，主要是“积小胜为大胜”“积跬步以致千里”，最终达到长期目标。 有目的的练习是专注的，要想取得进步，必须完全把注意力集中在你的任务上。 有目的的练习包含反馈，不论你在努力做什么事情，都需要反馈来准确辨别你在哪些地方还有不足，以及怎么会存在这些不足。 有目的的练习需要走出舒适区，如果你从来不迫使自己走出舒适区，便永远无法进步。 遇到瓶颈怎么办 试着做不同的事情，而非更难的事情。不管什么障碍，越过它的最好办法是从不同方向去想办法。 并非到达极限，而是动机不足。有意义的正面反馈是保持动机的关键要素之一。 有目的的练习还不够，还需要特定的练习与训练方法，这种方法就是刻意练习。 二、大脑的适应能力大脑就像肌肉，越练越大，早在2000年，马圭尔就发表了关于“伦敦出租车司机”的研究成果，在出租车司机的大脑之中，海马体的后部比普通人更大，司龄越长的改部位也越大。 大脑拥有无限的适应力，对这种适应能力最早的观察结果，在一些研究中多次出现，这些研究着眼于盲人或者聋哑人的大脑怎样“重新布线”，当盲人使用触摸布莱叶点字进行阅读时，看到大脑中处理部分还是视觉皮层。其结果告诉我们，大脑的结构和功能并不是固定不变的，它会根据你对它的运用尔改变。 走出舒适区的重要性，人类的身体有一种偏爱稳定性的倾向，被迫走出舒适区后，身体开始响应那些变化，目的是重新建立体内平衡。 练习改变大脑结构，对音乐家的研究表明，音乐训练以各种不同方式改变了大脑的结构和运行，使人们的音乐演奏能力进一步增强。经常性的训练使大脑中受到训练挑战的区域发生改变，大脑通过自身重新布线的方式来适应这些挑战，增强其执行那些挑战的能力。","tags":[{"name":"读书","slug":"读书","permalink":"https://joecnn.github.io/wiki-site/tags/读书/"}],"categories":[{"name":"阅读学习","slug":"阅读学习","permalink":"https://joecnn.github.io/wiki-site/categories/阅读学习/"}]},{"title":"深度阅读","date":"2018-04-19T16:00:00.000Z","path":"wiki/阅读学习/深度阅读/","text":"7S阅读法一、快速阅读阶段 明确阅读的目的，阅读的内容和速度 【S1 提问】自我梳理，提出阅读的几个问题 我为什么要阅读它？ 它对我来说有多重要？ 我要获得哪些信息？ 我打算把阅读获得的知识用在哪些方面？ 【S2 速览】关键词法，快速了解文本内容，通过段落首末句快速了解文本大概 二、深度阅读阶段 找出关键词句，总结作者表达的中心思想，整理论述结构并用自己的语言重构 【S3 融合】摘录法，快速阅读后通过摘录关键词句搭建全文的框架 【S4 重构】复述法，用自己的语言复述文本的内容 【S5 回忆】提问法，通过定期自我提问，回忆文本框架 三、书为我用阶段 通过延伸、交流来加深对阅读的理解 【S6 延伸】横向、纵向扩展，通过阅读相关资料，扩展知识的广度和深度 【S7 交流】笔记法，通过与好友交流、分享与实际应用中得到新的思路","tags":[{"name":"读书","slug":"读书","permalink":"https://joecnn.github.io/wiki-site/tags/读书/"}],"categories":[{"name":"阅读学习","slug":"阅读学习","permalink":"https://joecnn.github.io/wiki-site/categories/阅读学习/"}]},{"title":"Docker Mysql","date":"2018-03-07T13:30:00.000Z","path":"wiki/技术开发/devops/docker/docker mysql/","text":"docker mysql docker 下启动 mysql 实例步骤 1. 查询镜像 images123$ docker images$ docker search mysql$ docker pull mysql 2. 创建数据进行映射12$ mkdir -p /var/docker_data/mysql/data$ chmod 775 -R /var/docker_data/mysql 3. 运行容器1$ docker run -p 3306:3306 --name mysql-dev -v /var/docker_data/mysql/data/:/var/lib/mysql -e MYSQL_ROOT_PASSWORD=asdf123456 -e MYSQL_USER=root -d mysql 4. 验证是否已启动1$ docker ps","tags":[{"name":"docker","slug":"docker","permalink":"https://joecnn.github.io/wiki-site/tags/docker/"}],"categories":[{"name":"技术开发","slug":"技术开发","permalink":"https://joecnn.github.io/wiki-site/categories/技术开发/"},{"name":"devops","slug":"技术开发/devops","permalink":"https://joecnn.github.io/wiki-site/categories/技术开发/devops/"},{"name":"docker","slug":"技术开发/devops/docker","permalink":"https://joecnn.github.io/wiki-site/categories/技术开发/devops/docker/"}]},{"title":"系统并发用户估算","date":"2018-01-31T16:00:00.000Z","path":"wiki/技术开发/分布式/系统并发用户估算/","text":"泊松分布并发用户与最大并发用户计算： 1.平均并发用户数C = n*L/T n 为login session数 L 为login session平均时长 T 为考察时长 2.并发峰值用户数C’≈ C + 3*√200 3.吞吐量F=VU * R / T F为吞吐量 VU表示虚拟用户个数 R表示每个虚拟用户发出的请求数 T表示性能测试所用的时间 R = T / TS 例：假设有一个OA系统，该系统有3000个用户，平均每天大约有400个用户要访问该系统，对一个典型用户来说，一天之内用户从登录到退出该系统的平均时间为4小时，在一天的时间内，用户只在8小时内使用该系统。 根据公式计算得：C = 400 4/8 = 200C’≈200+3 √200 = 242","tags":[{"name":"分布式","slug":"分布式","permalink":"https://joecnn.github.io/wiki-site/tags/分布式/"}],"categories":[{"name":"技术开发","slug":"技术开发","permalink":"https://joecnn.github.io/wiki-site/categories/技术开发/"},{"name":"分布式","slug":"技术开发/分布式","permalink":"https://joecnn.github.io/wiki-site/categories/技术开发/分布式/"}]},{"title":"Git介绍","date":"2017-12-25T16:00:00.000Z","path":"wiki/技术开发/devops/git/Git介绍/","text":"Git是一个分布式的版本控制系统，最初由Linus Torvalds编写，用作Linux内核代码的管理。在推出后，Git在其它项目中也取得了很大成功，尤其是在Ruby社区中。 一、Git 特点直接记录快照，而非差异，只关心文件数据的整体是否发生变化，并不保存这些前后变化的差异数据。实际上，Git 更像是把变化的文件作快照后，记录在一个微型的文件系统中。每次提交更新时，它会纵览一遍所有文件的指纹信息并对文件作一快照，然后保存一个指向这次快照 的索引。为提高性能，若文件没有变化，Git 不会再次保存，而只对上次保存的快照作一链接。 近乎所有操作都是本地执行Git 绝大多数操作都只访问本地文件和资源，因为 Git 在本地磁盘上就保存着所有当前项目的历史更新，所以处理起来速度飞快。而到需要上传到远程仓库或获取远程仓库上内容时才需联网。 时刻保持数据完整性：Git 在保存前，所有数据都要进行校验计算，并将结果做为数据的唯一标识。 多数操作仅添加数据：Git 操作大多数仅仅是把数据添加到数据库，一旦提交快照就完全不用担心数据丢失。 二、Git 文件的三种状态 已提交(commited)：表示已被安全的保存到Git仓库中。 已修改(modified)：表示已修改了某个文件，但还未提交保存。 已暂存(staged)：表示已经把已修改的文件放到下次提交时要保存的清单中。 三、Git 工作流程 在工作目录中修改某些文件。 对修改后的文件进行快照，然后保存到暂存区域。 提交更新，将保存在暂存区域的文件快照永久转储到 Git 目录中","tags":[{"name":"git","slug":"git","permalink":"https://joecnn.github.io/wiki-site/tags/git/"}],"categories":[{"name":"技术开发","slug":"技术开发","permalink":"https://joecnn.github.io/wiki-site/categories/技术开发/"},{"name":"devops","slug":"技术开发/devops","permalink":"https://joecnn.github.io/wiki-site/categories/技术开发/devops/"},{"name":"git","slug":"技术开发/devops/git","permalink":"https://joecnn.github.io/wiki-site/categories/技术开发/devops/git/"}]},{"title":"Git基本操作","date":"2017-12-25T16:00:00.000Z","path":"wiki/技术开发/devops/git/Git基本操作/","text":"1. 取得 Git 仓库有两种取得 Git 项目仓库的方法，第一种是在现存的目录下，通过初始化文件夹来创建新的 Git 仓库：1git init 第二种是克隆仓库项目，Git 支持许多数据传输协议(git://, http(s)://, ssh)：1$ git clone [url] new_project_name 2. 检查当前文件状态要确定文件状态，可以使用 git status 命令。文件状态分为： untracked files: 未跟踪 changes to be committed: 已暂存 changed but not updated: 已修改，未暂存 3. 追踪文件使用命令 git add 开始跟踪文件或文件夹：1$ git add README git add 命令是个多功能命令，根据目标文件的状态不同，此命令的效果也不同：可以用它开始跟踪新文件，或者把已跟踪的文件放到暂存区，还能用于合并时把有冲突的文件标记为已解决状态等。 4. 忽略文件当某些文件无需纳入 Git 管理时，可以创建一个名为 .gitignore 的文件，列出要忽略的文件模式。文件 .gitignore 的格式如下：123456# 此为注释 – 将被 Git 忽略 *.a # 忽略所有 .a 结尾的文件 !lib.a # 但 lib.a 除外 /TODO # 仅仅忽略项目根目录下的 TODO 文件，不包括 subdir/TODO build/ # 忽略 build/ 目录下的所有文件 doc/*.txt # 会忽略 doc/notes.txt 但不包括 doc/server/arch.txt 对现有项目添加 git gitignore 不生效的解决办法： .gitignore 不生效解决方法 5. 查看已暂存和未暂存的变更内容如果想要查看具体修改内容，可以使用 git diff 命令：1$ git diff 此命令比较的是当前目录文件和暂存区快照之间的差异，也就是被修改后没有暂存的内容。如果想要查看暂存起来的文件和上次提交时的文件的差异，可以使用：12$ git diff --cached $ git diff --staged #v1.6.1+ 6. 提交更新当将所有文件加入暂存区后可以提交了，运行命令：1$ git commit -m \"提交已经暂存(add)过的变更\" 给 git commit 加上 -a 选项，跳过暂存步骤，自动将所有已跟踪过的文件提交：1$ git commit -a -m '直接提交变更，跳过暂存步骤' 7. 撤消操作某些时候，提交完后才发现漏了几个文件，或者提交信息写错了，修改最后一次提交，可以使用 –amend 选项重新提交：1$ git commit --amend -m \"修改备注信息\" 某些时间，不小心将不需要提交的文件使用了 git add，要取消暂存文件，可以使用以下命令：1$ git reset HEAD file_name 如果对文件修改没必要，需要撤销对文件的修改，可以使用：1$ git checkout -- file_name 注意此条命令会取消文件所有修改，回到上次提交的状态，需要谨慎使用。 8. 移除文件移除文件，并从工作目录中删除文件，也支持正则匹配：1$ git rm -f file_name 另一种情况，从 Git 中移除跟踪，但仍然保留文件，使用 –cached 选项即可：1$ git rm --cached file_name 9. 移动文件重命名或移动文件，都可以使用以下命令：1$ git mv file_from file_to 其实运行 git mv 就相当于运行下面三条命令：123$ mv README.txt README $ git rm README.txt $ git add README 10. 查看提交历史想回顾下提交历史记录，可以使用 git log 命令查看。123456789-p : 显示每次提交的内容差异 -2：仅显示最近两次更新 --stat : 仅显示简要的增改行数统计 --pretty : 自定义显示风格 =oneline : 放在一行显示 =format:\"%h - %an, %ar : %s\" 格式输出 --since=2.weeks : 最近时间段 --author=xx : 作者 --committer=xx : 提交者 使用图形化工具查看历史提交记录，使用命令 gitk 即可。 11. 远程仓库的使用要查看当前配置有哪些远程仓库，可以使用 git remote 命令，会列出每个远程仓库：1$ git remote -v 要添加一个新的远程仓库，需要在 github 上创建一个空的仓库，指定一个简单的名字，以便引用，运行 git remote add [shortname] [url] 创建本地仓库:1$ git remote add repository_name git://github.com/joecnn/imgcompact.git 从远程仓库拉取数据到本地，但并不合并到当前工作分支：1$ git fetch [remote-name] 使用 git pull 命令拉取远程仓库并合并到本地分支：1$ git pull [remote-name] 推送数据到远程仓库：1$ git push origin master 查看远程仓库信息：1$ git remote show origin 远程仓库的删除和重命名：12$ git remote rename old_name new_name $ git remote rm rp_name","tags":[{"name":"git","slug":"git","permalink":"https://joecnn.github.io/wiki-site/tags/git/"}],"categories":[{"name":"技术开发","slug":"技术开发","permalink":"https://joecnn.github.io/wiki-site/categories/技术开发/"},{"name":"devops","slug":"技术开发/devops","permalink":"https://joecnn.github.io/wiki-site/categories/技术开发/devops/"},{"name":"git","slug":"技术开发/devops/git","permalink":"https://joecnn.github.io/wiki-site/categories/技术开发/devops/git/"}]},{"title":"Git安装","date":"2017-12-25T16:00:00.000Z","path":"wiki/技术开发/devops/git/Git安装/","text":"1. 从源码安装Git的工作需要调用 curl, zlib, openssl, expat, libconv 等库，所以需要先安装这些依赖工具。1$ yum install curl-devel expat-devel gettext-devel openssl-devel zlib-devel 之后从Git官方站点下载源码：http://git-scm.com/download 进行编译安装：1234$ tar -zxf git-1.7.2.2.tar.gz $ cd git-1.7.2.2 $ make prefix=/usr/local all $ sudo make prefix=/usr/local install 2. 在 Linux 上安装如果要在 Linux 上安装预编译好的 Git 安装包，使用系统提供的包管理工具即可：12$ yum install git-core $ apt-get install git-core 3. 在 Mac 上安装在 Mac 上安装有两种方式，一种是下载 Git 安装工具，下载地址： http://git-scm.com/download/mac 另一种是通过 MacPorts 安装，用下面的命令安装：1$ sudo port install git-core +svn +doc +bash_completion +gitweb 4. 在 Windows 上安装在 Windows 上安装非常简单，只要下载 Git 安装工具，下载地址： http://git-scm.com/download/win 5. 初始配置一般在新的系统上，首先要配置 Git 的工作环境，Git 提供了 git config 的工具，用于配置和读取工作环境变量，这些变量可以存放在以下三个不同地方： /etc/gitconfig 文件：对系统中所有用户都适用，若使用 git config 时用 –system 选项，读写的就是这个文件。 ~/.gitconfig 文件：用户目录下仅用于用户，若使用 git config 时用 –global 选项，读写的就是这个文件。 当前项目的 git 目录中的配置文件：这里的配置仅仅对当前项目有效。每一个级别的配置都会覆盖上层的相同配置。 第一个要配置的是账户名称和邮箱地址：12$ git config --global user.name \"joecnn\" $ git config --global user.email \"starworking@126.com \" 配置文本编辑器：1$ git config --global core.editor vim 配置差异分析工具：1$ git config --global merge.tool vimdiff 6. 查看配置信息1$ git config --list 7. 获取帮助12$ git help $ man git-","tags":[{"name":"git","slug":"git","permalink":"https://joecnn.github.io/wiki-site/tags/git/"}],"categories":[{"name":"技术开发","slug":"技术开发","permalink":"https://joecnn.github.io/wiki-site/categories/技术开发/"},{"name":"devops","slug":"技术开发/devops","permalink":"https://joecnn.github.io/wiki-site/categories/技术开发/devops/"},{"name":"git","slug":"技术开发/devops/git","permalink":"https://joecnn.github.io/wiki-site/categories/技术开发/devops/git/"}]},{"title":"gitignore不生效解决方法","date":"2017-11-17T16:00:00.000Z","path":"wiki/技术开发/devops/git/gitignore不生效解决方法/","text":"在git中如果想忽略掉某个文件，不让这个文件提交到版本库中，可以使用修改根目录中 .gitignore 文件的方法（如无，则需自己手工建立此文件）。这个文件每一行保存了一个匹配的规则例如： 1234567# 此为注释 – 将被 Git 忽略*.a # 忽略所有 .a 结尾的文件!lib.a # 但 lib.a 除外/TODO # 仅仅忽略项目根目录下的 TODO 文件，不包括 subdir/TODObuild/ # 忽略 build/ 目录下的所有文件doc/*.txt # 会忽略 doc/notes.txt 但不包括 doc/server/arch.txt 但是有时候在项目开发过程中，想把某些目录或文件加入忽略规则，按照上述方法定义后发现并未生效，原因是.gitignore只能忽略那些原来没有被track的文件，如果某些文件已经被纳入了版本管理中，则修改.gitignore是无效的。那么解决方法就是先把本地缓存删除（改变成未track状态），然后再提交： 123git rm -r --cached .git add .git commit -m 'update .gitignore'","tags":[{"name":"git","slug":"git","permalink":"https://joecnn.github.io/wiki-site/tags/git/"}],"categories":[{"name":"技术开发","slug":"技术开发","permalink":"https://joecnn.github.io/wiki-site/categories/技术开发/"},{"name":"devops","slug":"技术开发/devops","permalink":"https://joecnn.github.io/wiki-site/categories/技术开发/devops/"},{"name":"git","slug":"技术开发/devops/git","permalink":"https://joecnn.github.io/wiki-site/categories/技术开发/devops/git/"}]},{"title":"常见问题收集","date":"2017-10-16T16:00:00.000Z","path":"wiki/技术开发/linux/常见问题收集/","text":"此文档为使用linux过程中遇到的各种坑，以及跳出坑的方法。 1. 启动时提示 systemd[1]: Failed to load SELinux policy. freezing. 无法进入系统此问题为设置错了 SELinux 配置，在启动时按e键修改 grub，在标签 fi 内核参数中增加一个参数1selinux=0 后按 ctrl+x 重启可进入，再修改 ==/etc/selinux/config==12SELINUX=disabledSELINUXTYPE=target 2. centos7 端口已开放，也开放了iptables防火墙，还是无法访问此问题为 centos7 已默认使用 firewall 为防火墙！1234567891011#关闭防火墙systemctl stop firewalld.service#禁止防火墙开机启动systemctl disable firewalld.service#查看防火墙开放端口firewall-cmd --list-ports#开放端口firewall-cmd --zone=public --add-port=80/tcp --permanent","tags":[{"name":"linux","slug":"linux","permalink":"https://joecnn.github.io/wiki-site/tags/linux/"}],"categories":[{"name":"技术开发","slug":"技术开发","permalink":"https://joecnn.github.io/wiki-site/categories/技术开发/"},{"name":"linux","slug":"技术开发/linux","permalink":"https://joecnn.github.io/wiki-site/categories/技术开发/linux/"}]},{"title":"Oracle 分区表","date":"2017-07-31T16:00:00.000Z","path":"wiki/技术开发/存储/oracle/Oracle 分区表/","text":"一、分区表&emsp;&emsp;随着表的不断增大，对于新纪录的增加、查找、删除等的维护也更加困难。对于数据库中的超大型表，可通过把它的数据分成若干个小表，从而简化数据库的管理活动。对于每一个简化后的小表，我们称为一个单个的分区。 对于分区的访问，我们不需要使用特殊的SQL查询语句或特定的DML语句，而且可以单独的操作单个分区，而不是整个表。同时也可以将不同分区的数据放置到不同的表空间。 对于外部应用程序来说，虽然存在不同的分区，且数据位于不同的表空间，但逻辑上仍然是一张表，可以使用SQL*Loader，IMPDP，EXPDP，Import，Export等工具来装载或卸载分区表中的数据。 二、何时进行分区 当表达到GB大小且继续增长 需要将历史数据和当前的数据分开单独处理，比如历史数据仅仅需要只读，而当前数据则实现DML 三、分区的特性 共性：不同的分区之间必须有相同的逻辑属性，比如表名，列名，数据类型，约束等 个性：各个分区可以有不同的物理属性，比如pctfree, pctused, and tablespaces 独立性：即使某些分区不可用，其他分区仍然可用 特殊性：含有 LONG、LONGRAW 数据类型的表不能进行分区 四、分区的优点1、提高查询性能：只需要搜索特定分区，而非整张表，提高查询速度2、节约维护时间：单个分区的数据装载，索引重建，备份，维护等将远小于整张表的维护时间3、节约维护成本：可以单独备份和恢复每个分区4、均衡I/O：将不同的分区映射到不同的磁盘以平衡I/O，提高并发 五、Oracle 分区类型 范围分区、散列分区、列表分区、组合分区 1. Range 分区：行映射到基于列值范围的分区Range 分区，又成为范围分区，基于分区键值的范围将数据映射到所建立的分区。创建分区时必须指定以下内容： 分区方法：range 分区列 标识分区边界的分区描述 使用 Range 分区的时候，要记住几条规则： 每个分区都包含 VALUES LESS THAN 字名，定义了分区的上层边界。任何等于和大于分区键值的二进制值都被添加到下一个高层分区中。 所有的分区，除了第一个，如果低于VALUES LESS THAN所定义的下层边界，都放在前面的分区中。 MAXVALUE 可以用来定义最高层的分区。MAXVALUE 表示了虚拟的无限值 示例： 123456789101112131415161718192021222324252627-- 创建分区create table sal_range (salesman_id number(5), salesman_name varchar2(30), sales_amount number(10), sales_date date) partition by range (sales_date) --创建基于日期的范围分区并存储到不同的表空间 ( partition sal_jan2000 values less than(to_date('02/01/2000', 'DD/MM/YYYY')) tablespace sal_range_jan2000, partition sal_feb2000 values less than(to_date('03/01/2000', 'DD/MM/YYYY')) tablespace sal_range_feb2000, partition sal_mar2000 values less than(to_date('04/01/2000', 'DD/MM/YYYY')) tablespace sal_range_mar2000, partition sal_apr2000 values less than(to_date('05/01/2000', 'DD/MM/YYYY')) tablespace sal_range_apr2000 );-- 查询分区中数据select * from r partition (p1)-- 添加分区ALTER TABLE r add partition p5 values less than (xxx ) tablespace xx;-- 查看分区表信息 SELECT table_name,partition_name,subpartition_count,tablespace_name,user_stats FROM user_tab_partitions; 2. Hash分区：散列分区Hash 分区是根据分区字段的 Hash 值进行的散列分区，在下面这种情况下，使用hash分区比range分区更好： 事先不知道需要将多少数据映射到给定范围的时候 分区的范围大小很难确定，或者很难平衡的时候 Range 分区使数据得到不希望的聚集时 性能特性，如并行DML、分区剪枝和分区连接很重要的时候 创建散列分区时，必须指定以下信息 分区方法：hash 分区列 分区数量或单独的分区描述 创建hash分区有两种方法：一种方法是指定分区数量，另一种方法是指定分区的名字，但两者不能同时指定。 123456789101112-- 方法一：指定分区数量create table dept2 (deptno number,deptname varchar2(32))partition by hash(deptno) partitions 4;-- 方法二：指定分区的名字create table sales_hash (salesman_id number(5), salesman_name varchar2(30), sales_amount number(10), week_no number(2))partition by hash (salesman_id) partitions 4store in (data1,data2,data3,data4) --指定表空间 3. List分区：列表分区List分区可以控制如何将行映射到分区中去，可以定义具体值指定到具体的分区。不同于Range分区和Hash分区： Range 分区与分区相关联，为分区列假设了一个值的自然范围，故不可能将该值的范围以外的分区组织到一起 Hash 分区时不允许对数据的划分进行控制，因为系统使用的是散列函数来划分数据的 List 分区的优点在于按照自然的方式将无序和不相关的数据集合分组 List 分区不支持多列分区，如果将表按列分区，那么分区键就只能有表的一个单独列组成 List分区时必须指定的以下内容： 分区方法：list 分区列 分区描述，每个描述指定一串文字值(值的列表),它们是分区列(它们限定将被包括在分区中的行)的离散值 示例： 1234567891011121314151617-- 创建分区create table sales_list (salesman_id number(5), salesman_name varchar2(30), sales_state varchar2(20), sales_amount number(10), sales_date date)partition by list (sales_state)( partition sales_west values ('California','Hawaii') tablespace x, partition sales_east values ('New York','Virginia') tablespace y, partition sales_central values ('Texas','Illinois') tablespace z, partition sales_other values(DEFAULT) tablespace o);-- 添加分区alter table sales3 add partition hk values ('HK') tablespace xx 4. Composite Partitioning：合成分区、组合分区组合分区使用 Range 方法分区，在每个子分区中使用 Hash 方法进行再分区。组合分区比 Range 分区更容易管理，充分使用了 Hash 分区的并行优势。组合分区支持历史数据和条块数据两者。创建组合分区时，需要指定如下内容： 分区方法：range 分区列 标识分区边界的分区描述 子分区方法：hash 子分区列 每个分区的子分区数量，或子分区的描述 示例： 12345678910111213141516171819202122232425262728293031-- 创建分区 Range分区，Hash子分区create table sales_composite (salesman_id number(5), salesman_name varchar2(30), sales_amount number(10), sales_date date)partition by range(sales_date)subpartition by hash(salesman_id) subpartitions 4store in (tbs1,tbs2,tbs3,tbs4)( partition sales_jan2000 values less than(to_date('02/01/2000','DD/MM/YYYY')), partition sales_feb2000 values less than(to_date('03/01/2000','DD/MM/YYYY')), partition sales_mar2000 values less than(to_date('04/01/2000','DD/MM/YYYY')));--Range分区，List子分区create table T_TRACK ( N_TRACK_ID NUMBER(20) NOT NULL, C_COMP_CDE VARCHAR2(6), T_TRACK_TM DATE NOT NULL, C_CAR_NO VARCHAR2(50) )partition by range(T_TRACK_TM)subpartition by list(C_COMP_CDE)( partition P_2009_11 values less than (to_date('2009-12-01','yyyy-MM-dd')) ( subpartition P_2009_11_P1013 values('P1013') )); 六、分区相关操作 添加分区： 12345678alter table T_TRACK add partition P_2005_04values less than(to_date('2005-05-01','yyyy-MM-dd'))( subpartition P_2005_04_P1013 values('P1013'), subpartition P_2005_04_P1013 values('P1014'), subpartition P_2005_04_P1013 values('P1015'), subpartition P_2005_04_P1013 values('P1016')) 删除分区： 1alter table T_TRACK drop partition p_2005_04; 添加子分区： 123alter table T_TRACKmodify partition P_2005_01add subpartition P_2005_01_P1017 values('P1017'); 删除子分区： 1alter table T_TRACK drop subpartition p_2005_01_p1017; 截断一个分区表中的一个分区的数据： 12345--这种方式会使全局分区索引无效alter table sales3 truncate partition sp1--这种方式全局分区索引不会无效 alter table sales3 truncate partition sp1 update indexes 截断分区表的子分区： 1alter table comp truncate subpartition sub1 截断带有约束的分区表 12345678--禁用约束alter table sales disable constraint dname_sales1--截断分区alter table sales truncate partitoin dec--启用约束alter table sales enable constraint dname_sales1 查看一个表是不是分区表 1select table_name,partitioned from user_tables; 将一个表的分区从一个表空间移动到另一个表空间： 12345678910111213--查看分区在哪个表空间SELECT TABLE_OWNER,TABLE_NAME,PARTITION_NAME,TABLESPACE_NAME, SUBPARTITION_COUNT FROM DBA_TAB_PARTITIONS WHERE TABLE_OWNER='SCOTT';--移动分区alter table sales move partiton sp1 tablespace tp;--检查是否移动成功SELECT TABLE_OWNER,TABLE_NAME,PARTITION_NAME,TABLESPACE_NAME,SUBPARTITION_COUNT FROM DBA_TAB_PARTITIONS WHERE TABLE_OWNER='SCOTT';--移动表空间后，要重建索引，否则索引会变得无效alter index xxx rebuild 合并分区： 12--合并后的分区名，不能是边界值较低的那个alter table sales3 merge partitons sp1,sp3 into partition sp3 与分区表相关的数据字典视图： DBA_TAB_PARTITIONS DBA_IND_PARTITIONS DBA_TAB_SUBPARTITIONS DBA_IND_SUBPARTITIONS 参考文档：[1] Oracle 关于分区的在线文档：http://download.oracle.com/docs/cd/B19306_01/server.102/b14220/partconc.htm#sthref2604","tags":[{"name":"oracle","slug":"oracle","permalink":"https://joecnn.github.io/wiki-site/tags/oracle/"}],"categories":[{"name":"技术开发","slug":"技术开发","permalink":"https://joecnn.github.io/wiki-site/categories/技术开发/"},{"name":"存储","slug":"技术开发/存储","permalink":"https://joecnn.github.io/wiki-site/categories/技术开发/存储/"},{"name":"oracle","slug":"技术开发/存储/oracle","permalink":"https://joecnn.github.io/wiki-site/categories/技术开发/存储/oracle/"}]},{"title":"Access 表结构查询","date":"2017-07-07T09:00:00.000Z","path":"wiki/技术开发/存储/Access 表结构查询/","text":"获取所有表12345var dt = conn.GetOleDbSchemaTable(OleDbSchemaGuid.Tables, null);var query = dt.AsEnumerable() .Where(x =&gt; x.Field&lt;string&gt;(&quot;TABLE_TYPE&quot;) == &quot;TABLE&quot; || x.Field&lt;string&gt;(&quot;TABLE_TYPE&quot;) == &quot;VIEW&quot;) .Select(x =&gt; x.Field&lt;string&gt;(&quot;TABLE_NAME&quot;)); 获取表字段1select * from table_name where 1&lt;&gt;1 获取表主键12DataTable dt = conn.GetOleDbSchemaTable(OleDbSchemaGuid.Primary_Keys, new object[] &#123; null, null, tableName &#125;);","tags":[{"name":"access","slug":"access","permalink":"https://joecnn.github.io/wiki-site/tags/access/"}],"categories":[{"name":"技术开发","slug":"技术开发","permalink":"https://joecnn.github.io/wiki-site/categories/技术开发/"},{"name":"存储","slug":"技术开发/存储","permalink":"https://joecnn.github.io/wiki-site/categories/技术开发/存储/"}]},{"title":"Mssql 表结构查询","date":"2017-04-06T09:00:00.000Z","path":"wiki/技术开发/存储/mssql/Mssql 表结构查询/","text":"获取所有表1234SELECT TABLE_NAME FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_SCHEMA='data_base' AND (TABLE_TYPE='BASE TABLE' OR TABLE_TYPE='VIEW') 获取表字段名1select * from table_name where 1&lt;&gt;1 获取表主键1234567891011121314SELECT col.name AS ColumnName , col.max_length AS DataLength , col.is_nullable AS IsNullable , t.name AS DataType , ( SELECT TOP 1 ind.is_primary_key FROM sys.index_columns ic LEFT JOIN sys.indexes ind ON ic.object_id = ind.object_id AND ic.index_id = ind.index_id AND ind.name LIKE 'PK_%' WHERE ic.object_id = obj.object_id AND ic.column_id = col.column_id ) AS IsPrimaryKey FROM sys.objects objINNER JOIN sys.columns col ON obj.object_id = col.object_idLEFT JOIN sys.types t ON t.user_type_id = col.user_type_id WHERE obj.name = 'table_name' 获取表字段详细信息123456789101112131415161718192021222324252627SELECT [表名] = Case When A.colorder=1 Then D.name Else '' End, [表说明] = Case When A.colorder=1 Then isnull(F.value,'') Else '' End, [字段序号] = A.colorder, [字段名] = A.name, [字段说明] = isnull(G.[value],''), [标识] = Case When COLUMNPROPERTY( A.id,A.name,'IsIdentity')=1 Then '√'Else '' End, [主键] = Case When exists(SELECT 1 FROM sysobjects Where xtype='PK' and parent_obj=A.id and name in ( SELECT name FROM sysindexes WHERE indid in( SELECT indid FROM sysindexkeys WHERE id = A.id AND colid=A.colid))) then '√' else '' end, [类型] = B.name, [占用字节数] = A.Length, [长度] = COLUMNPROPERTY(A.id,A.name,'PRECISION'), [小数位数] = isnull(COLUMNPROPERTY(A.id,A.name,'Scale'),0), [允许空] = Case When A.isnullable=1 Then '√'Else '' End, [默认值] = isnull(E.Text,'') FROM syscolumns A Left Join systypes B On A.xusertype=B.xusertype Inner Join sysobjects D On A.id=D.id and D.xtype='U' and D.name&lt;&gt;'dtproperties' Left Join syscomments E on A.cdefault=E.id Left Join sys.extended_properties G on A.id=G.major_id and A.colid=G.minor_id Left Join sys.extended_properties F On D.id=F.major_id and F.minor_id=0 where d.name='table_name'Order By A.id,A.colorder","tags":[{"name":"mssql","slug":"mssql","permalink":"https://joecnn.github.io/wiki-site/tags/mssql/"}],"categories":[{"name":"技术开发","slug":"技术开发","permalink":"https://joecnn.github.io/wiki-site/categories/技术开发/"},{"name":"存储","slug":"技术开发/存储","permalink":"https://joecnn.github.io/wiki-site/categories/技术开发/存储/"},{"name":"mssql","slug":"技术开发/存储/mssql","permalink":"https://joecnn.github.io/wiki-site/categories/技术开发/存储/mssql/"}]},{"title":"MySQL 表结构查询","date":"2017-04-05T16:00:00.000Z","path":"wiki/技术开发/存储/mysql/MySQL 表结构查询/","text":"获取所有表1234SELECT TABLE_NAME FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_SCHEMA='&#123;0&#125;' AND (TABLE_TYPE='BASE TABLE' OR TABLE_TYPE='VIEW') 获取表字段名1select * from table_name where 1&lt;&gt;1 获取表主键列12345678910111213SELECT k.TABLE_NAME, k.COLUMN_NAME, k.ORDINAL_POSITION, c.DATA_TYPE, c.COLUMN_TYPE FROM INFORMATION_SCHEMA.TABLE_CONSTRAINTS t JOIN INFORMATION_SCHEMA.KEY_COLUMN_USAGE kUSING (constraint_name,table_schema,table_name) JOIN INFORMATION_SCHEMA.COLUMNS cUSING (column_name,table_schema,table_name) WHERE t.constraint_type='PRIMARY KEY' AND t.table_schema='data_base' AND t.table_name='table_name'","tags":[{"name":"mysql","slug":"mysql","permalink":"https://joecnn.github.io/wiki-site/tags/mysql/"}],"categories":[{"name":"技术开发","slug":"技术开发","permalink":"https://joecnn.github.io/wiki-site/categories/技术开发/"},{"name":"存储","slug":"技术开发/存储","permalink":"https://joecnn.github.io/wiki-site/categories/技术开发/存储/"},{"name":"mysql","slug":"技术开发/存储/mysql","permalink":"https://joecnn.github.io/wiki-site/categories/技术开发/存储/mysql/"}]},{"title":"MySQL 树形结构查询","date":"2017-04-05T16:00:00.000Z","path":"wiki/技术开发/存储/mysql/MySQL 树形结构查询/","text":"在 中可以使用 ```Hierarchical Queries``` 通过 ```CONNECT BY``` 查询当前节点下所有节点，但在 ```mysql``` 中没有此功能，需要使用函数或者存储过程进行查询。12345678910#### 0. 数据准备&gt; 树形结构，单侧树可以看做链表结构。```sql-- 树形结构create table treeNodes ( id int primary key, nodename varchar(20), pid int ); 1. 函数获得子节点12345678910111213141516171819202122-- 查询树状节点函数CREATE FUNCTION `getChildLst`(rootId INT) RETURNS varchar(1000) BEGIN DECLARE sTemp VARCHAR(1000); DECLARE sTempChd VARCHAR(1000); SET sTemp = '$'; SET sTempChd =cast(rootId as CHAR); WHILE sTempChd is not null DO SET sTemp = concat(sTemp,',',sTempChd); SELECT group_concat(id) INTO sTempChd FROM treeNodes WHERE FIND_IN_SET(pid,sTempChd)&gt;0; END WHILE; RETURN sTemp;END-- 通过节点 rootid 查询该节点下的子节点树： select getChildLst(1);-- 使用 FIND_IN_SET 配合查询节点信息： select * from treeNodes where FIND_IN_SET(id, getChildLst(1)); 优点： 简单、没有递归层数的限制(123456789101112131415161718192021222324252627282930313233343536373839404142- **缺点：** 查询结果 ```returns varchar(1000)``` 受字符长度限制，无法支持无限层树。#### 2. 临时表与递归存储过程```sql-- 用于查询、初始化临时表的存储过程 CREATE PROCEDURE showChildList (IN rootId INT) BEGIN CREATE TEMPORARY TABLE IF NOT EXISTS tmpLst ( `sno` int primary key auto_increment COMMENT &apos;排序&apos;, `id` int COMMENT &apos;节点id&apos;, `depth` int COMMENT &apos;节点深度&apos;); DELETE FROM tmpLst; CALL createChildLst(rootId,0); SELECT tmpLst.*, treeNodes.* FROM tmpLst,treeNodes WHERE tmpLst.id=treeNodes.id ORDER BY tmpLst.sno; END; -- 构建树结构存储过程 CREATE PROCEDURE createChildLst (IN rootId INT,IN nDepth INT) BEGIN DECLARE done INT DEFAULT 0; DECLARE b INT; DECLARE cur1 CURSOR FOR SELECT id FROM treeNodes where pid=rootId; DECLARE CONTINUE HANDLER FOR NOT FOUND SET done = 1; insert into tmpLst values (null, rootId, nDepth); OPEN cur1; FETCH cur1 INTO b; WHILE done=0 DO CALL createChildLst(b,nDepth+1); FETCH cur1 INTO b; END WHILE; CLOSE cur1; END; -- 调用查询节点 call showChildLst(1); 优点： 可以更灵活处理、显示层数，并可以按照树的遍历顺序显示结构。 缺点： 递归层数限制255 3. 中间表和存储过程 由于 不允许在同一语句中对临时表多次引用，只能用普通表来实现，需要负责清理这个表。 123456789101112131415161718192021222324252627282930313233``` sql-- 构建树结构存储过程DROP PROCEDURE IF EXISTS showTreeNodes;CREATE PROCEDURE showTreeNodes (IN rootid INT)BEGIN DECLARE tLevel int ; DROP TABLE IF EXISTS tmpList; CREATE TABLE tmpList ( `id` int, `level` int, `sort` varchar(8000) ); Set tLevel = 0 ; INSERT into tmpList SELECT id, `tLevel`, ID FROM treeNodes WHERE PID = rootid; WHILE ROW_COUNT()&gt;0 DO SET tLevel = tLevel + 1 ; INSERT into tmpList SELECT A.ID, tLevel ,concat(B.sort, A.ID) FROM treeNodes A, tmpList WHERE A.PID=B.ID AND B.`level` = tLevel - 1 ; END WHILE; END;-- 调用存储过程，生成树结构CALL showTreeNodes(0);-- 查询结果SELECT concat(SPACE(B.`level`*2),&apos;+--&apos;,A.nodename) FROM treeNodes A,tmpList B WHERE A.ID=B.ID ORDER BY B.sort; 优点： 可以显示层数、可以按照树的遍历顺序得到结果、没有递归最大层数限制。 缺点： 由于 msql 中对临时表的限制，只能使用普通表，需要做后续数据清理工作。","tags":[{"name":"mysql","slug":"mysql","permalink":"https://joecnn.github.io/wiki-site/tags/mysql/"}],"categories":[{"name":"技术开发","slug":"技术开发","permalink":"https://joecnn.github.io/wiki-site/categories/技术开发/"},{"name":"存储","slug":"技术开发/存储","permalink":"https://joecnn.github.io/wiki-site/categories/技术开发/存储/"},{"name":"mysql","slug":"技术开发/存储/mysql","permalink":"https://joecnn.github.io/wiki-site/categories/技术开发/存储/mysql/"}]},{"title":"Git标签","date":"2017-03-30T16:00:00.000Z","path":"wiki/技术开发/devops/git/Git标签/","text":"Git 可以对某一时间点上的版本打上标签，经常用于某个软件版本（比如v1.0等）。 一、列出标签12# -l 表示匹配(pattern)1.0.*的标签git tag -l 'v1.0.*' 二、新建标签Git 的标签分为两类： 轻量级(lightweight)的标签，指向特定提交的引用，就像个不会变化的==分支==。 含附注(annotated)的标签，实际上是存储在仓库中的一个==独立对象==。 12345# 无参数默认创建一个轻量级标签git tag v1.0.1# -a 表示创建一个含附注的标签git tag -a v1.0.1 -m '1.0.1版本' 三、查看标签信息1git tag show v1.0 四、签署和验证标签使用私钥签署(GPG)标签，需要使用对应公钥进行验证。12345# -s 使用私钥进行GPG签署git tag -s v1.0 -m '1.0版本'# -v 使用公钥进行验证git tag -v v1.0 五、推送标签到远端仓库添加完标签后，需要推送到远程仓库，其他人才能获取到标签信息。12345# 推送指定标签git push origin v1.0# 推送所有标签git push origin --tags","tags":[{"name":"git","slug":"git","permalink":"https://joecnn.github.io/wiki-site/tags/git/"}],"categories":[{"name":"技术开发","slug":"技术开发","permalink":"https://joecnn.github.io/wiki-site/categories/技术开发/"},{"name":"devops","slug":"技术开发/devops","permalink":"https://joecnn.github.io/wiki-site/categories/技术开发/devops/"},{"name":"git","slug":"技术开发/devops/git","permalink":"https://joecnn.github.io/wiki-site/categories/技术开发/devops/git/"}]},{"title":"PowerDesigner导出表结构","date":"2016-12-25T16:00:00.000Z","path":"wiki/技术开发/工具/PowerDesigner 导出表结构/","text":"123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106'****************************************************************************** '* File: pdm2excel.txt '* Title: pdm export to excel '* Purpose: To export the tables and columns to Excel '* Model: Physical Data Model '* Objects: Table, Column, View '* Author: ziyan '* Created: 2012-05-03 '* Version: 1.0 '****************************************************************************** Option Explicit Dim rowsNum rowsNum = 0 '----------------------------------------------------------------------------- ' Main function '----------------------------------------------------------------------------- ' Get the current active model Dim Model Set Model = ActiveModel If (Model Is Nothing) Or (Not Model.IsKindOf(PdPDM.cls_Model)) Then MsgBox \"The current model is not an PDM model.\" Else ' Get the tables collection '创建EXCEL APP dim beginrow DIM EXCEL, SHEET set EXCEL = CREATEOBJECT(\"Excel.Application\") EXCEL.workbooks.add(-4167)'添加工作表 EXCEL.workbooks(1).sheets(1).name =\"test\" set sheet = EXCEL.workbooks(1).sheets(\"test\") ShowProperties Model, SHEET EXCEL.visible = true '设置列宽和自动换行 sheet.Columns(1).ColumnWidth = 20 sheet.Columns(2).ColumnWidth = 40 sheet.Columns(4).ColumnWidth = 20 sheet.Columns(5).ColumnWidth = 20 sheet.Columns(6).ColumnWidth = 15 sheet.Columns(1).WrapText =true sheet.Columns(2).WrapText =true sheet.Columns(4).WrapText =true End If '----------------------------------------------------------------------------- ' Show properties of tables '----------------------------------------------------------------------------- Sub ShowProperties(mdl, sheet) ' Show tables of the current model/package rowsNum=0 beginrow = rowsNum+1 ' For each table output \"begin\" Dim tab For Each tab In mdl.tables ShowTable tab,sheet Next if mdl.tables.count &gt; 0 then sheet.Range(\"A\" &amp; beginrow + 1 &amp; \":A\" &amp; rowsNum).Rows.Group end if output \"end\" End Sub '----------------------------------------------------------------------------- ' Show table properties '----------------------------------------------------------------------------- Sub ShowTable(tab, sheet) If IsObject(tab) Then Dim rangFlag rowsNum = rowsNum + 1 ' Show properties Output \"================================\" sheet.cells(rowsNum, 1) = \"实体名\" sheet.cells(rowsNum, 2) =tab.name sheet.cells(rowsNum, 3) = \"\" sheet.cells(rowsNum, 4) = \"表名\" sheet.cells(rowsNum, 5) = tab.code sheet.Range(sheet.cells(rowsNum, 5),sheet.cells(rowsNum, 6)).Merge rowsNum = rowsNum + 1 sheet.cells(rowsNum, 1) = \"属性名\" sheet.cells(rowsNum, 2) = \"说明\" sheet.cells(rowsNum, 3) = \"\" sheet.cells(rowsNum, 4) = \"字段中文名\" sheet.cells(rowsNum, 5) = \"字段名\" sheet.cells(rowsNum, 6) = \"字段类型\" '设置边框 sheet.Range(sheet.cells(rowsNum-1, 1),sheet.cells(rowsNum, 2)).Borders.LineStyle = \"1\" sheet.Range(sheet.cells(rowsNum-1, 4),sheet.cells(rowsNum, 6)).Borders.LineStyle = \"1\" Dim col ' running column Dim colsNum colsNum = 0 for each col in tab.columns rowsNum = rowsNum + 1 colsNum = colsNum + 1 sheet.cells(rowsNum, 1) = col.name sheet.cells(rowsNum, 2) = col.comment sheet.cells(rowsNum, 3) = \"\" sheet.cells(rowsNum, 4) = col.name sheet.cells(rowsNum, 5) = col.code sheet.cells(rowsNum, 6) = col.datatype next sheet.Range(sheet.cells(rowsNum-colsNum+1,1),sheet.cells(rowsNum,2)).Borders.LineStyle = \"2\" sheet.Range(sheet.cells(rowsNum-colsNum+1,4),sheet.cells(rowsNum,6)).Borders.LineStyle = \"2\" rowsNum = rowsNum + 1 Output \"FullDescription: \" + tab.Name End If End Sub","tags":[],"categories":[{"name":"技术开发","slug":"技术开发","permalink":"https://joecnn.github.io/wiki-site/categories/技术开发/"},{"name":"工具","slug":"技术开发/工具","permalink":"https://joecnn.github.io/wiki-site/categories/技术开发/工具/"}]},{"title":"Hadoop quickstart","date":"2016-12-15T16:00:00.000Z","path":"wiki/技术开发/分布式/Hadoop quickstart/","text":"1. Hadoop 是什么？Hadoop是一个分布式系统架构，由Apache基金会基于Java开发。用户可以在不了解分布式底层细节的情况下，开发分布式程序，充分利用集群的威力高速运算和存储。 2. Hadoop 能做什么？Hadoop 可以解决海量数据的存储、海量的数据分析。例如：Yahoo! 的垃圾邮件识别和过滤、用户特征建模；Amazon.com（亚马逊）的协同过滤推荐系统；Facebook的Web日志分析；Twitter、LinkedIn的人脉寻找系统；淘宝商品推荐系统、淘宝搜索中的自定义筛选功能……这些应用都使用到Hadoop及其相关技术。 3. Hadoop 的构成3.1 HDFS 分布式文件系统HDFS有着高容错性的特点，并且设计用来部署在低廉的硬件上。而且它提供高传输率来访问应用程序的数据，适合那些有着超大数据集的应用程序。HDFS是一个 master/slave 的结构，在 master 上只运行一个 NameNode，而在每一个 slave 上运行一个 DataNode。 NameNode负责: 接受用户请求. 维护文件系统目录结构. 管理文件与Block间关系，Block与DataNode之间关系.(默认Block块大小为64M ) DataNode负责: 存储文件. 文件被分成Block存储在磁盘上. 为保证数据安全，文件会有多个副本. 3.2 MapReduce 并行计算框架MapReduce 是 Google 的一项重要技术, 它是一个编程模型，用于进行大数据计算，通常采用的处理手法是并行计算。MapReduce 是一个 master/slave 的结构，在 master上只运行一个 JobTracker ，而在每一个slave上运行一个 TaskTracker。 JobTracker 负责: 接收客户提交的计算. 把计算分配给 TaskTracker 执行. 监控 TaskTracker 的执行情况. TaskTracker 负责: 执行 JobTracker 分配的任务. 4. Hadoop 的特点 扩容能力：通过增加节点，扩容方便可靠. 成本低：可以通过普通机器组成服务器集群来处理数据. 高效率：通过分发数据，可以在数据所在节点上并行处理数据. 可靠性：可以自动维护数据的多个副本，并在任务失败后自动重新部署计算任务 5. 安装 Hadoop环境准备 安装 Jdk 下载 Apache Hadoop 解压并设置环境变量1234$ tar -zxvf hadoop-1.1.2.tar.gz$ export HADOOP_HOME=/usr/local/hadoop/hadoop-1.1.2 $ export PATH=.:$HADOOP_HOME/bin/:$JAVA_HOME/bin/:$PATH $ source /etc/profile 配置 修改 $hadoop_home/conf/hadoop-env.sh JAVA_HOME 配置 修改 $hadoop_home/conf/core-size.xml 123456789101112&lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.default.name&lt;/name&gt; &lt;value&gt;hdfs://joecnn:9000&lt;/value&gt; &lt;description&gt;your hostname&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/usr/local/hadoop/hadoop-1.1.2/tmp&lt;/value&gt; &lt;description&gt;your hadoop home&lt;/description&gt; &lt;/property&gt; &lt;/configuration&gt; 修改 $hadoop_home/conf/hdfs-site.xml 12345678910111213141516171819202122&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.permissions&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; ``` 4. 修改 `修改 $hadoop_home/conf/mapred-site.xml````xml&lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapred.job.tracker&lt;/name&gt; &lt;value&gt;chenquan:9001&lt;/value&gt; &lt;description&gt;change your hostname&lt;/description&gt; &lt;/property&gt; &lt;/configuration&gt; 启动 Hadoop 格式化 HDFS 文件系统 hadoop namenode -format 启动 start-all.sh start-all.sh 启动所有的Hadoop守护。包括namenode, datanode, jobtracker, tasktrack stop-all.sh 停止所有的Hadoop start-mapred.sh 启动Map/Reduce守护。包括Jobtracker和Tasktrack stop-mapred.sh 停止Map/Reduce守护 start-dfs.sh 启动Hadoop DFS守护.Namenode和Datanode stop-dfs.sh 停止DFS守护 验证: jps 5个进程，namenode，datanode，jobtracker，tasktracker，secondarynamenodehadoop:50070/ 访问 hdfs webserver 页面hadoop:50030/ 访问 mapreduce webserver 页面 去除启动警告 查看 start-all.sh -&gt; hadoop-config.sh 修改 /etc/profile 增加 export HADOOP_HOME_WARN_SUPPRESS=1 source /etc/profile 刷新环境变量 验证： start-all.sh / stop-all.sh","tags":[{"name":"hadoop","slug":"hadoop","permalink":"https://joecnn.github.io/wiki-site/tags/hadoop/"},{"name":"分布式","slug":"分布式","permalink":"https://joecnn.github.io/wiki-site/tags/分布式/"}],"categories":[{"name":"技术开发","slug":"技术开发","permalink":"https://joecnn.github.io/wiki-site/categories/技术开发/"},{"name":"分布式","slug":"技术开发/分布式","permalink":"https://joecnn.github.io/wiki-site/categories/技术开发/分布式/"}]},{"title":"《 黑暗之魂2 原罪贤者 》白金攻略","date":"2016-07-17T16:00:00.000Z","path":"wiki/Playing/黑暗之魂2/","text":"丧失的记忆创建完人物自动获得奖杯。 弱者誓约如蜜村里和坐在高层大石碑边上的索丹对话，签订青教誓约获得。 强者誓约如蜜村大石板处签订霸者誓约获得。 最后的巨人打倒巨人陨落之森BOSS获得奖杯。 黑暗灵魂之始游戏中第一次死亡获得奖杯。 更衣通往虚影森林前的石化女解封后，给她换装获得奖杯。 深思首级虚影森林进入白雾区域贴左边走的废墟温格，和他说话获得奖杯。 守护誓约打败巨火塔远古的猎人BOSS后，与栏杆男对话签订誓约获得。 罪人的营火杀死罪人塔BOSS点燃始源篝火获得。 钟卫誓约月亮钟楼遗忘囚笼选择随从房间的篝火，下楼梯会有咒缚者出现。填上法洛斯之石开门后与边上的矮人侍从对话签订誓约。太阳钟楼熔铁城前篝火楼梯上去，与边上的矮人侍从对话签订誓约都可以解开奖杯。 太阳誓约流油谷左边岔道尽头，废墟雕像处签订契约获得奖杯。 被弃置的人们巨人森林洞穴里的地图男对话完。陨落之森主塔的道具商人老奶奶对话完。密港法师智力达到8以上对话完。流油谷铁匠的女儿说话完。推土塔的梯子男只要打倒BOSS毒妃后他也会回如蜜。通往虚影森林前的石化女解封对话完。 熔铁的营火熔铁城打倒BOSS老铁王，点燃始源篝火获得奖杯。 鼠王誓约打败BOSS鼠王先锋出去后，会看见一只站立的老鼠对话获得奖杯。法洛斯门径试炼休息处篝火打败鼠王的试炼，BOSS另一个门外有个老鼠躲在洞里也可以签订誓约。 血族誓约打败狩猎森林BOSS刑吏查略特，篝火旁NPC需要憎恨之证才能开启对话签订誓约获得。（憎恨之证在打死鼠王先锋BOSS后一路下去的木台下面木箱里或者杀死在线入侵玩家获得） 锻造至高武器在铁匠处任意一把武器升到顶就可以获得奖杯。 溪谷的营火打败腐败物BOSS，点燃始源篝火获得奖杯。 辉石的营火打败BOSS公爵的夫雷伯亚，点燃始源篝火获得奖杯。 熟练工匠篝火麦道夫的工坊需要点燃烛台，捡到将熄余焰交给他。在他那强化武器消费14000魂以上，选择框里的说话选项解开奖杯，他还会送一个楔形石圆盘。 国王戒指打死不死陵庙BOSS王盾韦施塔德后，在老王徘徊的附近地上捡到戒指后解开奖杯。 远古之龙在卫龙巢穴最深处遇见古龙对话获得灰雾核心，解开奖杯。 镜之骑士打败王城BOSS后，解开奖杯获得。 黑暗誓约三个远古暗穴都要找到后才能和老头对话签订契约解开奖杯。一个在茫然遗迹会塌陷的地方。另一个在黑溪谷打死巨人废弃的钥匙开门里，最后是在王城门 前篝火左上角门内下去。 远古誓约龙祭坛2层找到化石蛋，交给熔铁城里的商人，对话后签订契约解开奖杯。 亚凯提拉需要玩家召唤白符鲁卡提耶至少三次BOSS战胜利后她幸存不死，才会在安迪尔馆前庭篝火处遇见鲁卡提耶。特别提醒如果不满足条件不要和她对话说完她就会消失，满足条件得到她的装备和正统骑士团大剑解开奖杯。鲁卡提耶四个对话地点：密港房间内。遗忘囚牢离塔篝火出去直走的房间内。流油谷BOSS贪婪的恶魔右手边布满毒瓶的房间内。黑溪谷篝火出来右边小道跳下去的平台里。可以召唤她打BOSS的地点：密港四臂守卫BOSS，熔铁城熔铁恶魔BOSS，罪人塔遗忘的罪人BOSS，黑溪谷腐败物BOSS。DLC3白王BOSS。 继承者游戏通关制作组字幕结束回到如蜜，解开奖杯。 泛克拉德拿了5个巨人魂可以给老王最大的伤害，一开始要多砍几刀才会触发BOSS战。打死他获得奖杯。 肢体动作大师向NPC学会所有的动作姿势解开奖杯。如蜜索丹学习——欢迎。海德巨火塔青之骑士学习——开战礼节。虚影森林温格之首学习——枭首。如蜜被门挡住的苍剑男学习——喜悦。狩猎森林不死牢篝火处救出克雷顿学习——握拳高举。梯子男古里甘学习——下跪。带如蜜村房间里猫npc买的细雨戒指，在茫然遗迹打死蝎女和蝎子男学习——活动肩膀。罪人塔石化男史垂德学习——看不起人。不死陵庙守墓人亚格德兰学习——求饶。进入王城透明宰相学习——我来。太阳契约学习——赞美太阳。巨人回忆王国队长学习——欢呼。 献身者任何一个誓约打到三级解开奖杯。誓约等级和贡献数二周目继承。 完成奇妙的地图和回到如蜜自己家里的地图男对话解开奖杯，需要点燃八个篝火：巨人陨落之森主塔篝火。罪人塔离塔篝火。熔铁城艾吉尔铁像篝火。黑溪谷入口篝火。辉石镇盖多勒下层篝火。不死陵庙死者废弃处篝火。龙祭坛入口篝火。王都多兰古雷德国王门 前篝火。 月光大剑需要玩家召唤白符凡荷特至少三次BOSS战胜利后他幸存不死，才会在巨人的回忆处遇见，对话后得到他的装备和苍之大剑解开奖杯。（添火打同一个BOSS三次是否可行，没有验证不确定。）苍剑男对话地点：首先在虚影森林入口遇见，解封石化女后务必回去再和他对话触发下部剧情。之后会在王城中层篝火处遇见他对话完。然后他会在巨人陨落之森的咒缚者附近坐着。最后他会去到巨人回忆里。可以召唤他BOSS战的地方：辉石镇徘徊的术士他的符在帐篷里。镜之骑士BOSS。巨人王BOSS。渴望王座BOSS。dlc深渊王冠污秽艾蕾娜BOSS。 要塞守护者他在巨人汪姆德的回忆里，第一次对话触发肢体语言，打败巨人王后再进去触发转赠奖杯获得。 守财奴买走所有能架起的梯子，然后对话解开奖杯。 习得所有魔法者得到所有魔法解开奖杯。（列表在2楼） 习得所有奇迹者得到所有奇迹解开奖杯。（列表在2楼） 习得所有咒术者得到所有咒术解开奖杯。（列表在2楼） 习得所有暗术者得到所有暗术解开奖杯。（列表在2楼） 黑暗灵魂恭喜白金达成。 元素碎片*13如蜜村井内击打石头获得。咒缚者平台尸体处。虚影森林魔女居处篝火右边木门里。海德巨火塔下去用怀念香木解开杂兵上楼梯后的栏杆处。遗忘囚笼通往铁匠左手边小道尽头。狩猎森林BOSS骷髅王右手边岔路进去，拉开铁门机关的地方。辉石镇滑绳下去右边下去一个房间里。废渊底层海德骑士楼梯上去的平台处捡到。王城打死查略特马后直走下去有两个女沙之魔法师的房间宝箱里。阿玛拉祭坛第二篝火河马守护的小路后面宝箱里。 贵人骨灰*5从巨火塔走楼梯下去，通往密港的左手铁箱内。狩猎森林需要从上面跳入的洞堡铁箱内。虚影森林魔女居处篝火左边有一个口可以跳到对面铁箱内。杀死鼠王先锋BOSS后一路跳到最下面，有爆破兵的尽头铁箱内。王城门 前篝火直走楼梯上去正对面扎因骑士守护的房间里。 关于全魔法全奇迹全咒术全暗术奖杯的说明，这次的原罪贤者全收集奖杯获得条件，还包括了3个dlc里能拿到的法术请务必注意这点。收集的攻略可以参考专区置顶的索引帖，这帖如有不明白或者错误的地方欢迎回帖我再修改解答。魔法修理的位置改到了茫然遗迹一个石化狮子后面的铁箱里。 DLC1深渊王冠进入方法黑溪谷右边有悬崖可以跳下去看到一扇门，再向下跳会看到巨人，打死掉落废弃钥匙。去如蜜村的平台，从梯子男最深的梯子下去上到对面的梯子，会有扇门打开得到DLC1的钥匙龙爪。DLC2钢铁王冠进入方法熔铁城关掉机关后，进门第一个喷火马头处捡到熔铁钥匙。去巨人陨落之森最后的巨人BOSS左手边开门，直走尽头的火蛙守护的尸体处捡到DLC2的钥匙重铁钥和反叛大盾。DLC3白王王冠进入方法王城国王门 前篝火出去右边第一扇门，进去的尸体上捡到冰之花钥匙。 二周目8小时 三周目2小时就过了方法是 一周目在dlc3 刷个冰刺洗点 50敏 剩下多余的都加信仰 冰刺雷附魔二周目正常打四物 这四个灵魂换四个法术 白金路上必须换的 给出一个比较效率又兼顾多周目体验的路线，如蜜-巨人-咒缚者-海德龙骑兵-下坑杀腐肉-咒缚者那里去牢笼杀三基友和罪人（三基友没必要跳过多周目非常简单，要跳过的话需要从密港那边拿钥匙开门走法洛斯浪费时间不推荐）-茫然遗迹蝎女-辉石镇蜘蛛-狩猎森林骷髅王-流油谷大虫-推土塔娜迦-熔铁双雄（多周目熔铁一定不要急，稳健清小怪才能真正节省时间）-王城龙骑镜骑-祭坛青蛙（依然要稳扎稳打，抱着一次过小怪的谨慎）-灵庙王盾拿戒指-安第尓看守龙-跳过龙穴直奔祭坛拿核心-巨人王-王座两连战 身上不要带魂进入三周目 先去巨人森林 老奶奶买一个怀念香木之后跳废渊 再捡一个香木 腐败物前面那个营火 用一个香木穿上刷魂套 戴+2银蛇 添火刷八九次腐败物（雷附魔冰刺非常好打） 再捏身上所有的魂 就够300w魂量了然后直接开冬庙去王城 买法术 白金！ 黑暗之魂2原罪学者 全法术获取地点一览魔法：灵魂箭：巨人森林老奶奶买到强力灵魂箭： 密港魔法师买到灵魂巨箭：密港魔法师买到强力灵魂巨箭：密港魔法师买到追踪灵魂箭：遗忘牢笼魔法师追踪灵魂巨箭：遗忘牢笼魔法师需要用三守卫的灵魂交换追踪灵魂块：辉石镇鸟姐追踪结晶灵魂块：阿马那祭坛，第二个篝火附近的死尸灵魂枪：不死陵墓商人结晶灵魂枪：二周目遗忘牢笼魔法师需要用远古白龙的灵魂交换冲击：密港魔法师买到乱射灵魂枪：密港魔法师买到灵魂降雹：遗忘牢笼魔法师需要用蝎子女的灵魂交换灵魂大剑：王城打过双龙骑士之后流程会到一间楼上又有个弓箭手的房间拉机关，在那个房间里。漩涡灵魂塊：不死陵墓商人灵魂闪光：王城打过镜子骑士到了电梯那个房间不要急着马上坐电梯，向左看。灵魂洪流：安达尔之馆有硫酸的地牢里法术武器：密港魔法师买到强力法术武器：很多地方都有，最简单的是黑溪谷可以拣到结晶法术武器：龙祭坛拿化石蛋那里，从窗户跳出去之后来到一个小平台，向右看。强力法术盾：王城某个宝箱或者找安达尔之馆那个NPC可以买到思乡：密港魔法师买到静音：遗忘牢笼打3守卫的房间有个挂在平台边缘的死尸有道具，那个平台上有个暗墙进去便是。控制坠落：辉石镇鸟姐隐形武器：这个很麻烦，要成为钟塔守卫契约然后打倒20个入侵钟塔的玩家，然后和钟塔矮子对话就会给你这个魔法，其实太阳钟楼有一个随机复活的红魂杀一次也算打倒一个入侵玩家。我来说一下怎么刷： 到太阳钟楼的篝火 爬上平台之后直走左拐就能遇见这个红魂，红魂不会有任何入侵提示，就是站在那边这么一个普通的敌人，样子是持双刀的战士。 这个敌人是随机刷出来的所以如果没看见这个敌人的话果断回到篝火处坐下刷新，大概刷个15~20次能出来一次的样子。 刷到钟楼矮子给你魔法为止，大概20次修理：茫然遗迹一个石化狮子后面的铁箱里光明照耀：这个很早就能拿到，忘记在哪里拿的了拟态：流油谷某个宝箱解放魔力：安达尔之馆雾门里的法师，忘记是做杀人道具任务还是从他手里买到的了灵魂一闪：DLC3白之王冠收束灵魂块：DLC1深渊王冠 奇迹：回复：海德灯塔拜金女买到中等回复：海德灯塔拜金女买到缩约大恢复：海德灯塔拜金女买到恢复：辉石镇教堂上去的商人阳光治愈：用复仇之眼入侵拜金女打倒后得到，复仇之眼在不死陵墓，那个可以放下来的桥旁边有个平台，掉下去的宝箱里。生命力涌现：海德灯塔拜金女买到生命洋溢：海德灯塔拜金女买到阳光滋润：3周目王城前的幽灵执事治疗祈福：海德灯塔拜金女买到原力：海德灯塔拜金女买到诸神之怒：3周目王城前的幽灵执事原力四射：到处都有的东西，实在找不到辉石镇商人有卖苍天雷鸣：辉石镇商人有卖雷枪：海德灯塔拜金女买到巨雷枪：后期可以在遗忘牢笼法师商人买到，或者从黑溪谷拿到深渊钥匙之后可以在如蜜那个井下到最下面开一个门的宝箱里。阳光之枪：太阳誓约并且给30个太阳徽章得到，太阳徽章online随便在哪里丢个白印使劲刷就可以了，offline虚影森林签霸者契约添火刷大鹰骑士镇魂：流油谷大波妹光辉雷球：二周目可以在遗忘牢笼法师商人用古王的灵魂换到法术防护：狩猎森林第一个篝火前坐椅子的法师强化法术防护：王城前的幽灵执事返回：海德灯塔拜金女买到引导之语：海德灯塔拜金女买到金石之誓：遗忘牢笼法师商人用韦斯塔德的灵魂换到察觉敌意：遗忘牢笼法师商人伟大的抵抗：辉石镇商人有卖阳光之剑：遗忘牢笼法师商人或者阿马那祭坛靠近第二个篝火远处有个箱子惜别：DLC1深渊王冠分裂雷枪：DLC3白之王冠 咒术：火球：虚影森林石化妹子火弹：虚影森林石化妹子大火球：辉石镇商人有卖，或者巨人森林下面砍死火蜥蜴掉混沌大火球：3周目王城前的幽灵执事火焰风暴：不死人刑场的商人火焰大风暴：阿那马祭坛，第二个篝火往后有个房子，进去出来之后遇见一个肉搏法师敌人，遇见那个法师的地方向右走有个宝箱。混沌风暴：熔铁城第二个篝火之后那个大房间，看见西北方熔岩上那个宝箱没有?就在那里引燃火焰：虚影森林石化妹子引燃烈火：不死人刑场的商人横扫火焰：不死人刑场的商人毒雾：虚影森林石化妹子猛毒雾：遗忘牢笼法师商人用鼠王先锋灵魂交换喷射酸液：遗忘牢笼法师商人用鼠王试炼的灵魂交换漂浮火球：遗忘牢笼法师商人火锤术：遗忘牢笼法师商人封印太阳：安达尔之馆雾门里的法师，做任务得到火焰武器：二周目遗忘牢笼法师商人用古魔女之魂换到剧烈出汗：虚影森林石化妹子钢铁身躯：虚影森林石化妹子温暖的火：不死陵墓商人焚身火：太阳钟楼通过**之后进雾门宝箱便是火蛇：DLC2铁之王冠舞蹈之火：DLC2铁之王冠咆哮：DLC2铁之王冠，捡到12个煤炭新娘灵魂合并成一个灵魂后，到遗忘牢笼法师换取 暗术：暗珠：狩猎森林第一个篝火前坐椅子的法师黑暗飞沫：狩猎森林第一个篝火前坐椅子的法师暗毒雾：遗忘牢笼法师商人追踪众：遗忘牢笼法师商人死者活化：流油谷大波妹黑暗武器：狩猎森林第一个篝火前坐椅子的法师失望呢喃：圣人墓地，开法洛斯机关放下吊桥，过了吊桥后地上捡到反作用：遗忘牢笼法师商人需要巨人王的灵魂扭曲护壁：辉石镇商人有卖神经激烈迟钝：二周目遗忘牢笼法师商人需要远古死者灵魂生命残渣：辉石镇商人有卖暗黑风暴：狩猎森林第一个篝火前坐椅子的法师(似乎要后期?)灵魂共鸣：游戏早期某个宝箱，或者做暗黑契约的过程中会送强烈灵魂共鸣：游戏早期某个宝箱，或者做暗黑契约的过程中会送亢奋：完成暗黑契约所有任务(打倒暗天使)躯体共鸣：狩猎森林第一个篝火前坐椅子的法师武器共鸣：狩猎森林第一个篝火前坐椅子的法师吸精微光：遗忘牢笼法师商人需要暗黑潜伏者的灵魂深层沉默：狩猎森林第一个篝火前坐椅子的法师必然的和平足迹：DLC1深渊王冠暗之舞：DLC3白之王冠暗之大剑：DLC1深夜王冠回忆：DLC2铁之王冠","tags":[{"name":"攻略","slug":"攻略","permalink":"https://joecnn.github.io/wiki-site/tags/攻略/"}],"categories":[{"name":"Playing","slug":"Playing","permalink":"https://joecnn.github.io/wiki-site/categories/Playing/"}]},{"title":"Mssql 日期格式化","date":"2016-02-18T09:00:00.000Z","path":"wiki/技术开发/存储/mssql/Mssql 日期格式化/","text":"SQL中CONVERT的使用方法:格式:CONVERT(data_type,e­xpression[,style])说明:此样式一般在时间类型 (datetime, smalldatetime) 与字符串类型 (nchar, nvarchar, char, varchar) 相互转换的时候才用到. 123456789101112131415161718192021222324252627282930313233343536373839404142-- 例子:Select CONVERT(varchar(100), GETDATE(), 0): 05 16 2006 10:57AMSelect CONVERT(varchar(100), GETDATE(), 1): 05/16/06Select CONVERT(varchar(100), GETDATE(), 2): 06.05.16Select CONVERT(varchar(100), GETDATE(), 3): 16/05/06Select CONVERT(varchar(100), GETDATE(), 4): 16.05.06Select CONVERT(varchar(100), GETDATE(), 5): 16-05-06Select CONVERT(varchar(100), GETDATE(), 6): 16 05 06Select CONVERT(varchar(100), GETDATE(), 7): 05 16, 06Select CONVERT(varchar(100), GETDATE(), 8): 10:57:46Select CONVERT(varchar(100), GETDATE(), 9): 05 16 2006 10:57:46:827AMSelect CONVERT(varchar(100), GETDATE(), 10): 05-16-06Select CONVERT(varchar(100), GETDATE(), 11): 06/05/16Select CONVERT(varchar(100), GETDATE(), 12): 060516Select CONVERT(varchar(100), GETDATE(), 13): 16 05 2006 10:57:46:937Select CONVERT(varchar(100), GETDATE(), 14): 10:57:46:967**Select CONVERT(varchar(100), GETDATE(), 20): 2006-05-16 10:57:47**Select CONVERT(varchar(100), GETDATE(), 21): 2006-05-16 10:57:47.157Select CONVERT(varchar(100), GETDATE(), 22): 05/16/06 10:57:47 AM**Select CONVERT(varchar(100), GETDATE(), 23): 2006-05-16Select CONVERT(varchar(100), GETDATE(), 24): 10:57:47**Select CONVERT(varchar(100), GETDATE(), 25): 2006-05-16 10:57:47.250Select CONVERT(varchar(100), GETDATE(), 100): 05 16 2006 10:57AM**Select CONVERT(varchar(100), GETDATE(), 101): 05/16/2006**Select CONVERT(varchar(100), GETDATE(), 102): 2006.05.16Select CONVERT(varchar(100), GETDATE(), 103): 16/05/2006Select CONVERT(varchar(100), GETDATE(), 104): 16.05.2006Select CONVERT(varchar(100), GETDATE(), 105): 16-05-2006Select CONVERT(varchar(100), GETDATE(), 106): 16 05 2006Select CONVERT(varchar(100), GETDATE(), 107): 05 16, 2006Select CONVERT(varchar(100), GETDATE(), 108): 10:57:49Select CONVERT(varchar(100), GETDATE(), 109): 05 16 2006 10:57:49:437AMSelect CONVERT(varchar(100), GETDATE(), 110): 05-16-2006**Select CONVERT(varchar(100), GETDATE(), 111): 2006/05/16**Select CONVERT(varchar(100), GETDATE(), 112): 20060516Select CONVERT(varchar(100), GETDATE(), 113): 16 05 2006 10:57:49:513Select CONVERT(varchar(100), GETDATE(), 114): 10:57:49:547Select CONVERT(varchar(100), GETDATE(), 120): 2006-05-16 10:57:49Select CONVERT(varchar(100), GETDATE(), 121): 2006-05-16 10:57:49.700Select CONVERT(varchar(100), GETDATE(), 126): 2006-05-16T10:57:49.827Select CONVERT(varchar(100), GETDATE(), 130): 18 ???? ?????? 1427 10:57:49:907AMSelect CONVERT(varchar(100), GETDATE(), 131): 18/04/1427 10:57:49:920AM","tags":[{"name":"mssql","slug":"mssql","permalink":"https://joecnn.github.io/wiki-site/tags/mssql/"}],"categories":[{"name":"技术开发","slug":"技术开发","permalink":"https://joecnn.github.io/wiki-site/categories/技术开发/"},{"name":"存储","slug":"技术开发/存储","permalink":"https://joecnn.github.io/wiki-site/categories/技术开发/存储/"},{"name":"mssql","slug":"技术开发/存储/mssql","permalink":"https://joecnn.github.io/wiki-site/categories/技术开发/存储/mssql/"}]},{"title":"安装Python3","date":"2016-01-07T16:00:00.000Z","path":"wiki/技术开发/linux/安装Python3/","text":"1. 安装 Python 依赖包12$ yum groupinstall \"Development tools\"$ yum install zlib-devel bzip2-devel openssl-devel ncurses-devel sqlite-devel readline-devel tk-devel gdbm-devel db4-devel libpcap-devel xz-devel 2. 下载 Python3.5 源码包并编译123$ wget https://www.python.org/ftp/python/3.5.0/Python-3.5.0.tgz$ tar -zxvf Python-3.5.0.taz$ cd Python-3.5.0 3. 编译安装1234$ ./configure --prefix=/usr/local --enable-shared$ make $ make install$ ln -s /usr/local/bin/python3 /usr/bin/python3 4. 在运行 Python 前配置需要库12$ echo /usr/local/lib &gt;&gt; /etc/ld.so.conf.d/local.conf$ ldconfig 5. 检测安装1$ python3 --version 6. 移除编译 Python 安装的库12$ yum groupremove \"Development tools\"$ yum remove zlib-devel bzip2-devel openssl-devel ncurses-devel sqlite-devel readline-devel tk-devel gdbm-devel db4-devel libpcap-devel xz-devel 7. 设置别名1$ alias py=python3","tags":[{"name":"linux","slug":"linux","permalink":"https://joecnn.github.io/wiki-site/tags/linux/"}],"categories":[{"name":"技术开发","slug":"技术开发","permalink":"https://joecnn.github.io/wiki-site/categories/技术开发/"},{"name":"linux","slug":"技术开发/linux","permalink":"https://joecnn.github.io/wiki-site/categories/技术开发/linux/"}]},{"title":"网络配置","date":"2016-01-05T16:00:00.000Z","path":"wiki/技术开发/linux/网络配置/","text":"1. 检测是否已启动网卡1$ ifconfig &amp; ifup eth0 2. 修改对应网卡配置信息1$ vi /etc/sysconfig/network-scripts/ifcfg-eth0 123456789DEVICE=&quot;eth0&quot;HWADDR=&quot;00:0C:29:FD:FF:2A&quot;NM_CONTROLLED=&quot;yes&quot;ONBOOT=&quot;yes&quot;IPADDR=192.168.1.31NETMASK=255.255.255.0GATEWAY=192.168.1.1BOOTPROTO=staticDNS1=192.168.1.1 3. 重启网卡12$ service network restart $ /etc/init.d/network restart 4. 可以安装网络图形设置工具1$ yum install setuptool 5. 检测网络状态1$ service network status","tags":[{"name":"linux","slug":"linux","permalink":"https://joecnn.github.io/wiki-site/tags/linux/"}],"categories":[{"name":"技术开发","slug":"技术开发","permalink":"https://joecnn.github.io/wiki-site/categories/技术开发/"},{"name":"linux","slug":"技术开发/linux","permalink":"https://joecnn.github.io/wiki-site/categories/技术开发/linux/"}]},{"title":"安装gcc","date":"2016-01-05T16:00:00.000Z","path":"wiki/技术开发/linux/安装gcc/","text":"一、安装内核支持gcc1$ yum -y install gcc+ gcc-c++ 二、手动升级gcc1. 获取安装包12$ wget http://ftp.gnu.org/gnu/gcc/gcc-4.8.2/gcc-4.8.2.tar.bz2$ tar -jxvf gcc-4.8.2.tar.bz2 2. 下载编译需要的依赖项12$ cd gcc-4.8.2$ ./contrib/down_prerequiesites 3. 建立一个目录供编译输出的文件存放1$ mkdir gcc-build-4.8.2 4. 生成 Makefile 文件12$ cd gcc-build-4.8.2$ ../configure -enable-checking=release -enable-languages=c,c++ -disable-multilib 5. 编译, -j4选项是make对多核处理器的优化12$ make -j4$ yum -y install glibc-devel.i686 glibc-devel 6. 安装1$ make install 7. 检查gcc版本1$ gcc --version","tags":[{"name":"linux","slug":"linux","permalink":"https://joecnn.github.io/wiki-site/tags/linux/"}],"categories":[{"name":"技术开发","slug":"技术开发","permalink":"https://joecnn.github.io/wiki-site/categories/技术开发/"},{"name":"linux","slug":"技术开发/linux","permalink":"https://joecnn.github.io/wiki-site/categories/技术开发/linux/"}]},{"title":"Oracle 冷备启库","date":"2013-12-25T16:00:00.000Z","path":"wiki/技术开发/存储/oracle/Oracle 冷备启库/","text":"1、准备工作 查看数据版本是否跟库文件相同 查看数据文件是否都齐全 查看 init.ora 是否在（参数文件，也可以自己配置） 2、创建文件夹 在 oracle/admin 目录下创建 实例名文件夹。如：phoenix 在此文件夹中创建 adump、dpdump、pfile ,将 init.ora 放置在 pfile 文件夹中 在 fast_recovery_area 文件夹中创建实例名文件夹4. 在 oradata 文件夹中创建实例名文件夹 3、创建空实例1$ oradim -new -sid phoenix -startmode auto -pfile init.ora 4、创建密码文件1$ orapwd file=D:\\oracle\\product\\11.2.0\\dbhome_1\\database\\pwdphoenix.ora password=oracle; 5、进入创建好的实例12$ sqlplus /nolog $ conn zysys/zy2009phoenix as sysdba 6、生成spfile文件1$ create spfile from pfile='D:\\Oracle\\admin\\phoenix\\pfile\\init.ora'; 7、重建控制文件123456789101112131415161718$ CREATE CONTROLFILE REUSE SET DATABASE \"phoenix\" RESETLOGS NOARCHIVELOG MAXLOGFILES 16 MAXLOGMEMBERS 3 MAXDATAFILES 100 MAXINSTANCES 8 MAXLOGHISTORY 292 LOGFILE GROUP 1 'E:\\Work\\Data\\Orcl\\REDO01.LOG' SIZE 50M BLOCKSIZE 512, GROUP 2 'E:\\Work\\Data\\Orcl\\REDO02.LOG' SIZE 50M BLOCKSIZE 512, GROUP 3 'E:\\Work\\Data\\Orcl\\REDO03.LOG' SIZE 50M BLOCKSIZE 512 -- STANDBY LOGFILE DATAFILE 'E:\\Work\\Data\\Orcl\\SYSTEM01.DBF', 'E:\\Work\\Data\\Orcl\\SYSAUX01.DBF', 'E:\\Work\\Data\\Orcl\\UNDOTBS01.DBF', 'E:\\Work\\Data\\Orcl\\USERS01.DBF', ………………数据文件位置……………… CHARACTER SET ZHS16GBK; 8、启动数据库 alter database open; // alter database open resetlogs upgrade 小版本不同需要升级 若是选用noresetlogs选项, 若无法打开数据库, 则尝试执行recover database; 若是在线热备数据库文件,则还要把热备时刻的归档日志拷过来,恢复时用下列语法recover database using backup controlfile until cancel;写入日志文件 d：/xxx.log 创建临时表空间alter tablespace temp add tempfile &#39;C:\\Oracle\\database\\phoenix\\temp01.ora&#39; size 50M reuse autoextend on next 50M maxsize 500M; 小版本不同需要执行升级脚本oracle_home/rdbms/admin/catupgrd.sql； 重启数据库","tags":[{"name":"oracle","slug":"oracle","permalink":"https://joecnn.github.io/wiki-site/tags/oracle/"}],"categories":[{"name":"技术开发","slug":"技术开发","permalink":"https://joecnn.github.io/wiki-site/categories/技术开发/"},{"name":"存储","slug":"技术开发/存储","permalink":"https://joecnn.github.io/wiki-site/categories/技术开发/存储/"},{"name":"oracle","slug":"技术开发/存储/oracle","permalink":"https://joecnn.github.io/wiki-site/categories/技术开发/存储/oracle/"}]},{"title":"Oracle 导入导出","date":"2013-12-25T16:00:00.000Z","path":"wiki/技术开发/存储/oracle/Oracle 导入导出/","text":"可将整个数据库，用户，表导出备份，在一些包含大字段的表无法用sql插入时，很好用哦。 导出exp 123exp zysys/zy2009phoenix@phoenix_my file=e:\\phoenix.dmp full=y --完全导出 exp zysys/zy2009phoenix@phoenix_my file=e:\\zymbs.dmp owner=(zysys,zymbs) --按用户导出 exp zysys/zy2009phoenix@phoenix_my file=e:\\table.dmp tables=(ZYMBS.RHRSP_KPI_DICT,table2) --导出表 导入imp 123imp zysys/zy2009phoenix@phoenix file=c:\\orabackup\\hkbfull.dmp log=c:\\orabackup\\hkbimp.log full=y --完全导入 imp zysys/zy2009phoenix@phoenix_my fromuser=zymbs touser=zymbs FILE=E:\\zymbs.dmp ignore=y; --按用户 imp zysys/zy2009phoenix@phoenix file=e:\\table.dmp tables=(RHRSP_KPI_DICT) --导入表（不需要表空间名） 11g R2 导出空表oracle 11g r2 新增了一个参数：deferred_segment_creation, 如果这个参数设置为 true，你新建了一个表 T1，并且没有向其中插入数据，那么这个表不会立即分配 extent，也就是不占数据空间，只有当你 insert 数据后才分配空间。这样可以节省少量的空间。所以在导出时，空的表将不导出需要单独处理 1) 设置 deferred_segment_creation 参数为FALSESQL&gt; alter system set deferred_segment_creation=false; 2) 注意并且要重启数据库，让参数生效, 但是这样操作后只会生效于新表 3) 使用 allocate extent 可以为数据库对象分配ExtentSQL&gt; ALLOCATE EXTENT { SIZE integer [K | M] | DATAFILE &#39;filename&#39; | INSTANCE integer 对现有表的处理脚本 createsql.sql：1234567set heading off; set echo off; set feedback off; set termout on; spool C:\\allocate.sql; select 'alter table '||table_name||' allocate extent;' from user_tables where num_rows=0; spool off;","tags":[{"name":"oracle","slug":"oracle","permalink":"https://joecnn.github.io/wiki-site/tags/oracle/"}],"categories":[{"name":"技术开发","slug":"技术开发","permalink":"https://joecnn.github.io/wiki-site/categories/技术开发/"},{"name":"存储","slug":"技术开发/存储","permalink":"https://joecnn.github.io/wiki-site/categories/技术开发/存储/"},{"name":"oracle","slug":"技术开发/存储/oracle","permalink":"https://joecnn.github.io/wiki-site/categories/技术开发/存储/oracle/"}]},{"title":"Oracle 切换数据模式","date":"2013-12-25T16:00:00.000Z","path":"wiki/技术开发/存储/oracle/Oracle 切换数据模式/","text":"1alter datatbase datafile=F:\\phoenix\\zycure.ora offline drop ;","tags":[{"name":"oracle","slug":"oracle","permalink":"https://joecnn.github.io/wiki-site/tags/oracle/"}],"categories":[{"name":"技术开发","slug":"技术开发","permalink":"https://joecnn.github.io/wiki-site/categories/技术开发/"},{"name":"存储","slug":"技术开发/存储","permalink":"https://joecnn.github.io/wiki-site/categories/技术开发/存储/"},{"name":"oracle","slug":"技术开发/存储/oracle","permalink":"https://joecnn.github.io/wiki-site/categories/技术开发/存储/oracle/"}]},{"title":"Oracle 表空间","date":"2013-12-25T16:00:00.000Z","path":"wiki/技术开发/存储/oracle/Oracle 表空间/","text":"查看数据文件位置 1SELECT TABLESPACE_NAME,FILE_ID,BYTES,FILE_NAME FROM DBA_DATA_FILES; 表空间状态 1SELECT TABLESPACE_NAME,STATUS FROM DBA_TABLESPACES; 创建表空间 123456CREATE TABLESPACE \"ZYMBS\" NOLOGGING --不创建重做日志文件 DATAFILE 'E:\\WORK\\DATA\\PHOENIX\\ZYMBS01.ORA' SIZE 5M AUTOEXTEND ON --自动增长 NEXT 50M MAXSIZE 2048M --自动增长最大到2G EXTENT MANAGEMENT LOCAL; 创建用户 123CREATE USER zymbs IDENTIFIED BY zy2009phoenix DEFAULT TABLESPACE ZYMBS TEMPORARY TABLESPACE TEMP; 分发权限 1234567GRANT --系统权限 CREATE SESSION, CREATE ANY TABLE, CREATE ANY VIEW ,CREATE ANY INDEX, CREATE ANY PROCEDURE, ALTER ANY TABLE, ALTER ANY PROCEDURE, DROP ANY TABLE, DROP ANY VIEW, DROP ANY INDEX, DROP ANY PROCEDURE, SELECT ANY TABLE, INSERT ANY TABLE, UPDATE ANY TABLE, DELETE ANY TABLE TO ZYMBS; GRANT RESOURCE, CONNECT TO ZYMBS; --角色权限 删除表空间 1DROP TABLESPACE ZYMBS INCLUDING CONTENTS AND DATAFILES; 修改状态(ONLINE,OFFLINE,READONLY,READWRITE) 1ALTER TABLESPACE ZYMBS OFFLINE; 修改文件大小,添加文件 12ALTER TABLESPACE DATAFILE &apos;E:\\ZYMBS01.ORA&apos; RESIZE 10M; ALTER TABLESPACE ZYMBS ADD DATAFILE &apos;E:\\ZYMBS02.ORA&apos; SIZE 5M; 修改表空间数据文件的自动扩展性 1ALTER DATABASE DATAFIELE &apos;E:\\ZYMBS01.ORA&apos; AUTOEXTEND ON NEXT 5M MAXSIZE 50M; 重新指定表空间路径 1alter tablespace SIMPLE RENAME DATAFILE &apos;E:\\table2.dbf&apos; to &apos;D:\\table.dbf&apos;; 设置默认表空间 1. 查询(当前用户)默认表空间 select default_tablespace from user_users; 2. 修改默认表空间 alter database default tablespace SIMPLE; 如果是oracle 9i本语句不支持，但能让用户指向某表空间，具体做法如下： 例： alter user scott default tablespace users; (scott为用户名, users为表空间) 有时会出现表空间有存在的情况，这时一般都是以下几个原因造成的： 1、写错表空间名，我想的话这种机率较小。 2、回想一下，你在创建表空间时是否给表空间表加了双引号如： CREATE TABLESPACE &quot;Sample&quot; ……………… 如果是这样的话，你在修改默认表空间，写表空间名字的时候就要区分大小了，这个是非常重要的，并且还要加上双引号， 如：alter user scott default tablespace &quot;Sample&quot;;(scott为用户名)。","tags":[{"name":"oracle","slug":"oracle","permalink":"https://joecnn.github.io/wiki-site/tags/oracle/"}],"categories":[{"name":"技术开发","slug":"技术开发","permalink":"https://joecnn.github.io/wiki-site/categories/技术开发/"},{"name":"存储","slug":"技术开发/存储","permalink":"https://joecnn.github.io/wiki-site/categories/技术开发/存储/"},{"name":"oracle","slug":"技术开发/存储/oracle","permalink":"https://joecnn.github.io/wiki-site/categories/技术开发/存储/oracle/"}]}]}